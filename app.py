import streamlit as st
import requests
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
from datetime import datetime, timedelta
import json
import asyncio
import aiohttp
import time
import sqlite3
import os
from pathlib import Path
import hashlib
import joblib
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from ratelimit import limits, sleep_and_retry
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import io
from reportlab.lib.pagesizes import A4, letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from reportlab.lib.units import inch, cm
from reportlab.pdfgen import canvas
from reportlab.platypus import Image
from reportlab.platypus.flowables import Flowable
from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_JUSTIFY
import xlsxwriter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.set_page_config(
    page_title="CTA Article Recommender Pro",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å—Ç–∏–ª–∏ (–±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ)
st.markdown("""
<style>
    .main-header {
        font-size: 2.2rem;
        font-weight: 700;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.8rem;
    }
    
    .step-card {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        border-radius: 12px;
        padding: 18px;
        border-left: 4px solid #667eea;
        margin-bottom: 15px;
        box-shadow: 0 3px 5px rgba(0, 0, 0, 0.04);
    }
    
    .metric-card {
        background: white;
        border-radius: 10px;
        padding: 15px;
        box-shadow: 0 3px 10px rgba(0, 0, 0, 0.06);
        border: 1px solid #e0e0e0;
        height: 100%;
        min-height: 90px;
    }
    
    .metric-card h4 {
        font-size: 0.85rem;
        margin: 0 0 8px 0;
        color: #666;
    }
    
    .metric-card .value {
        font-size: 1.6rem;
        font-weight: 700;
        color: #333;
        line-height: 1.2;
    }
    
    .result-card {
        background: white;
        border-radius: 10px;
        padding: 15px;
        margin-bottom: 12px;
        border-left: 3px solid #4CAF50;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);
    }
    
    .compact-button {
        padding: 8px 16px !important;
        font-size: 0.9rem !important;
        margin: 5px 0 !important;
        border-radius: 6px !important;
    }
    
    .compact-textarea {
        font-size: 0.9rem !important;
        line-height: 1.4 !important;
    }
    
    .compact-select {
        font-size: 0.9rem !important;
    }
    
    .compact-slider {
        margin: 5px 0 !important;
    }
    
    .back-button {
        position: absolute;
        top: 10px;
        left: 10px;
        z-index: 100;
    }
    
    .progress-container {
        background: #f5f5f5;
        border-radius: 8px;
        height: 6px;
        margin: 20px 0;
        overflow: hidden;
    }
    
    .progress-bar {
        height: 100%;
        background: linear-gradient(90deg, #667eea, #764ba2);
        border-radius: 8px;
        transition: width 0.5s ease;
    }
    
    .step-indicator {
        display: flex;
        justify-content: space-between;
        margin: 15px 0;
        font-size: 0.85rem;
        color: #666;
    }
    
    .step-indicator .active {
        color: #667eea;
        font-weight: 600;
    }
    
    .filter-chip {
        display: inline-flex;
        align-items: center;
        padding: 4px 10px;
        margin: 2px;
        background: #e3f2fd;
        border-radius: 16px;
        font-size: 0.8rem;
        color: #1565c0;
    }
    
    .info-message {
        background: linear-gradient(135deg, #2196F315 0%, #0D47A115 100%);
        border-radius: 8px;
        padding: 12px;
        border-left: 3px solid #2196F3;
        font-size: 0.9rem;
        margin: 10px 0;
    }
    
    .warning-message {
        background: linear-gradient(135deg, #FF980015 0%, #EF6C0015 100%);
        border-radius: 8px;
        padding: 12px;
        border-left: 3px solid #FF9800;
        font-size: 0.9rem;
        margin: 10px 0;
    }
    
    .topic-card {
        background: white;
        border-radius: 8px;
        padding: 12px;
        margin-bottom: 8px;
        border: 1px solid #e0e0e0;
        cursor: pointer;
        transition: all 0.2s ease;
    }
    
    .topic-card:hover {
        border-color: #667eea;
        box-shadow: 0 2px 8px rgba(102, 126, 234, 0.1);
    }
    
    .topic-card.selected {
        background: linear-gradient(135deg, #667eea10 0%, #764ba210 100%);
        border-color: #667eea;
        border-left: 4px solid #667eea;
    }
    
    .dataframe th {
        padding: 8px 12px !important;
        font-size: 0.85rem !important;
    }
    
    .dataframe td {
        padding: 6px 12px !important;
        font-size: 0.85rem !important;
    }
    
    .download-buttons {
        display: flex;
        gap: 10px;
        margin: 15px 0;
    }
    
    .download-button {
        flex: 1;
    }
</style>
""", unsafe_allow_html=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAlex API
OPENALEX_BASE_URL = "https://api.openalex.org"
MAILTO = "your-email@example.com"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à email
POLITE_POOL_HEADER = {'User-Agent': f'CTA-App (mailto:{MAILTO})'}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ rate limit
RATE_LIMIT_PER_SECOND = 8
BATCH_SIZE = 50
CURSOR_PAGE_SIZE = 200
MAX_WORKERS_ASYNC = 3
MAX_RETRIES = 3
INITIAL_DELAY = 1
MAX_DELAY = 60

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
CACHE_DIR = Path("./cache")
CACHE_DB = CACHE_DIR / "openalex_cache.db"
CACHE_EXPIRY_DAYS = 30

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
CACHE_DIR.mkdir(exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
nltk.download('stopwords', quiet=True)
COMMON_WORDS = {
    'study', 'studies', 'research', 'paper', 'article', 'review', 'analysis', 'analyses',
    'investigation', 'investigations', 'effect', 'effects', 'property', 'properties',
    'performance', 'behavior', 'behaviour', 'characterization', 'characterisation',
    'synthesis', 'development', 'preparation', 'fabrication', 'application', 'applications',
    'method', 'methods', 'approach', 'approaches', 'result', 'results', 'discussion',
    'conclusion', 'conclusions', 'introduction', 'experimental', 'experiment', 'experiments',
    'measurement', 'measurements', 'observation', 'observations', 'technique', 'techniques',
    'technology', 'technologies', 'material', 'materials', 'system', 'systems',
    'process', 'processes', 'structure', 'structures', 'model', 'models',
    'based', 'using', 'used', 'use', 'high', 'low', 'temperature', 'temperatures',
    'pressure', 'different', 'various', 'several', 'important', 'significant',
    'novel', 'new', 'recent', 'current', 'potential', 'possible', 'first',
    'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth',
    'tenth', 'good', 'better', 'best', 'poor', 'higher', 'lower', 'strong',
    'weak', 'large', 'small', 'great', 'major', 'minor', 'main', 'primary',
    'secondary', 'critical', 'essential', 'general', 'specific', 'special',
    'particular', 'similar', 'different', 'various', 'several', 'multiple',
    'numerous', 'common', 'unusual', 'typical', 'atypical', 'standard',
    'advanced', 'basic', 'fundamental', 'theoretical', 'practical', 'experimental',
    'computational', 'numerical', 'analytical', 'theoretical', 'practical'
}

ALL_STOPWORDS = set(stopwords.words('english')).union(COMMON_WORDS)

# ============================================================================
# –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ù–ê –£–†–û–í–ù–ï SQLite
# ============================================================================

def init_cache_db():
    conn = sqlite3.connect(CACHE_DB)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS works_cache (
            doi TEXT PRIMARY KEY,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS topic_works_cache (
            topic_id TEXT,
            cursor_key TEXT,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME,
            PRIMARY KEY (topic_id, cursor_key)
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS topics_cache (
            topic_id TEXT PRIMARY KEY,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME
        )
    ''')
    
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_works_expires ON works_cache(expires_at)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic_works_expires ON topic_works_cache(expires_at)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_topics_expires ON topics_cache(expires_at)')
    
    conn.commit()
    conn.close()

def get_cache_key(prefix: str, key: str) -> str:
    return hashlib.md5(f"{prefix}:{key}".encode()).hexdigest()

@st.cache_resource
def get_db_connection():
    init_cache_db()
    return sqlite3.connect(CACHE_DB, check_same_thread=False)

def cache_work(doi: str, data: dict):
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=CACHE_EXPIRY_DAYS)
    
    cursor.execute('''
        INSERT OR REPLACE INTO works_cache (doi, data, expires_at)
        VALUES (?, ?, ?)
    ''', (doi, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_work(doi: str) -> Optional[dict]:
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM works_cache 
        WHERE doi = ? AND (expires_at IS NULL OR expires_at > ?)
    ''', (doi, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def cache_topic_works(topic_id: str, cursor_key: str, data: dict):
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=7)
    
    cursor.execute('''
        INSERT OR REPLACE INTO topic_works_cache (topic_id, cursor_key, data, expires_at)
        VALUES (?, ?, ?, ?)
    ''', (topic_id, cursor_key, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_topic_works(topic_id: str, cursor_key: str) -> Optional[dict]:
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM topic_works_cache 
        WHERE topic_id = ? AND cursor_key = ? 
        AND (expires_at IS NULL OR expires_at > ?)
    ''', (topic_id, cursor_key, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def cache_topic_stats(topic_id: str, data: dict):
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=30)
    
    cursor.execute('''
        INSERT OR REPLACE INTO topics_cache (topic_id, data, expires_at)
        VALUES (?, ?, ?)
    ''', (topic_id, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_topic_stats(topic_id: str) -> Optional[dict]:
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM topics_cache 
        WHERE topic_id = ? AND (expires_at IS NULL OR expires_at > ?)
    ''', (topic_id, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def clear_old_cache():
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫—É –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è SQLite
    now_str = datetime.now().isoformat(' ', 'seconds')
    
    cursor.execute('DELETE FROM works_cache WHERE expires_at <= ?', (now_str,))
    cursor.execute('DELETE FROM topic_works_cache WHERE expires_at <= ?', (now_str,))
    cursor.execute('DELETE FROM topics_cache WHERE expires_at <= ?', (now_str,))
    
    conn.commit()

# ============================================================================
# –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê –î–ò–ê–ü–ê–ó–û–ù–û–í –¶–ò–¢–ò–†–û–í–ê–ù–ò–ô
# ============================================================================

def parse_citation_ranges(range_str: str) -> List[Tuple[int, int]]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π.
    
    –ü—Ä–∏–º–µ—Ä—ã:
    "0" -> [(0, 0)]
    "0-3" -> [(0, 3)]
    "1,3-5" -> [(1, 1), (3, 5)]
    "0-1,3-4" -> [(0, 1), (3, 4)]
    "0,1,2,3" -> [(0, 0), (1, 1), (2, 2), (3, 3)]
    """
    ranges = []
    
    if not range_str or range_str.strip() == "":
        return [(0, 10)]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
    parts = range_str.split(',')
    
    for part in parts:
        part = part.strip()
        if '-' in part:
            # –î–∏–∞–ø–∞–∑–æ–Ω
            try:
                start, end = part.split('-')
                start = int(start.strip())
                end = int(end.strip())
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ end >= start –∏ –æ–±–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 0-10
                if start <= end and 0 <= start <= 10 and 0 <= end <= 10:
                    ranges.append((start, end))
                else:
                    logger.warning(f"Invalid range: {part}. Using default.")
                    ranges.append((0, 10))
            except ValueError:
                logger.warning(f"Could not parse range: {part}. Using default.")
                ranges.append((0, 10))
        else:
            # –û–¥–∏–Ω–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            try:
                value = int(part.strip())
                if 0 <= value <= 10:
                    ranges.append((value, value))
                else:
                    logger.warning(f"Value out of range: {value}. Using default.")
                    ranges.append((0, 10))
            except ValueError:
                logger.warning(f"Could not parse value: {part}. Using default.")
                ranges.append((0, 10))
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    unique_ranges = list(set(ranges))
    unique_ranges.sort(key=lambda x: x[0])
    
    return unique_ranges if unique_ranges else [(0, 10)]

def format_citation_ranges(ranges: List[Tuple[int, int]]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –≤ —á–∏—Ç–∞–µ–º—É—é —Å—Ç—Ä–æ–∫—É.
    """
    if not ranges:
        return "0-10"
    
    parts = []
    for start, end in ranges:
        if start == end:
            parts.append(str(start))
        else:
            parts.append(f"{start}-{end}")
    
    return ", ".join(parts)

# ============================================================================
# ASYNCIO + AIOHTTP
# ============================================================================

class OpenAlexAsyncClient:
    def __init__(self):
        self.session = None
        self.semaphore = asyncio.Semaphore(MAX_WORKERS_ASYNC)
        self.request_count = 0
        self.start_time = time.time()
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers=POLITE_POOL_HEADER,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    @retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=INITIAL_DELAY, max=MAX_DELAY),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def make_request(self, url: str) -> Optional[dict]:
        async with self.semaphore:
            elapsed = time.time() - self.start_time
            expected_time = self.request_count / RATE_LIMIT_PER_SECOND
            
            if elapsed < expected_time:
                wait_time = expected_time - elapsed
                await asyncio.sleep(wait_time)
            
            try:
                async with self.session.get(url) as response:
                    self.request_count += 1
                    
                    if response.status == 429:
                        retry_after = int(response.headers.get('Retry-After', 5))
                        logger.warning(f"Rate limited. Waiting {retry_after} seconds")
                        await asyncio.sleep(retry_after)
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=429
                        )
                    
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 404:
                        logger.warning(f"Resource not found: {url}")
                        return None
                    else:
                        logger.error(f"HTTP {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.warning(f"Timeout: {url}")
                raise
            except Exception as e:
                logger.error(f"Error: {url} - {str(e)}")
                raise
    
    async def fetch_works_by_dois_batch(self, dois: List[str]) -> List[Optional[dict]]:
        if not dois:
            return []
        
        cached_results = []
        uncached_dois = []
        
        for doi in dois:
            cached = get_cached_work(doi)
            if cached:
                cached_results.append(cached)
            else:
                uncached_dois.append(doi)
        
        if not uncached_dois:
            return cached_results
        
        logger.info(f"Fetching {len(uncached_dois)} works via batch API")
        
        doi_filter = "|".join(uncached_dois)
        url = f"{OPENALEX_BASE_URL}/works?filter=doi:{doi_filter}&per-page=200"
        
        try:
            data = await self.make_request(url)
            if data and 'results' in data:
                results = data['results']
                
                for work in results:
                    doi = work.get('doi', '').replace('https://doi.org/', '')
                    if doi:
                        cache_work(doi, work)
                
                doi_to_work = {w.get('doi', '').replace('https://doi.org/', ''): w for w in results}
                batch_results = []
                
                for doi in uncached_dois:
                    if doi in doi_to_work:
                        batch_results.append(doi_to_work[doi])
                    else:
                        try:
                            work_data = await self.fetch_single_work(doi)
                            batch_results.append(work_data)
                        except:
                            batch_results.append(None)
                
                return cached_results + batch_results
            else:
                return cached_results + [None] * len(uncached_dois)
                
        except Exception as e:
            logger.error(f"Batch fetch error: {str(e)}")
            return cached_results + [None] * len(uncached_dois)
    
    async def fetch_single_work(self, doi: str) -> Optional[dict]:
        cached = get_cached_work(doi)
        if cached:
            return cached
        
        url = f"{OPENALEX_BASE_URL}/works/https://doi.org/{doi}"
        data = await self.make_request(url)
        
        if data:
            cache_work(doi, data)
        
        return data
    
    async def fetch_works_by_topic_cursor(self, topic_id: str, max_results: int = 2000, 
                                         progress_callback=None) -> List[dict]:
        all_works = []
        cursor = "*"
        page_count = 0
        
        cache_key = f"{topic_id}_cursor_{cursor}"
        cached = get_cached_topic_works(topic_id, cache_key)
        
        if cached and len(cached) >= max_results:
            logger.info(f"Using cached data for topic {topic_id}")
            return cached[:max_results]
        
        logger.info(f"Fetching works for topic {topic_id} (max: {max_results})")
        
        try:
            while len(all_works) < max_results and cursor:
                page_count += 1
                
                url = (f"{OPENALEX_BASE_URL}/works?"
                      f"filter=topics.id:{topic_id}&"
                      f"per-page={CURSOR_PAGE_SIZE}&"
                      f"cursor={cursor}&"
                      f"sort=publication_date:desc")
                
                data = await self.make_request(url)
                
                if not data or 'results' not in data:
                    break
                
                works = data['results']
                if not works:
                    break
                
                all_works.extend(works)
                
                # Call progress callback
                if progress_callback:
                    progress = min(len(all_works) / max_results, 1.0)
                    progress_callback(progress, len(all_works), page_count)
                
                meta = data.get('meta', {})
                cursor = meta.get('next_cursor')
                
                logger.info(f"Page {page_count}: got {len(works)} works, total: {len(all_works)}")
                
                cache_key = f"{topic_id}_cursor_{cursor or 'end'}"
                cache_topic_works(topic_id, cache_key, all_works)
                
                if not cursor or page_count >= 10:
                    break
                
                await asyncio.sleep(0.5)
            
            result = all_works[:max_results]
            cache_topic_works(topic_id, "final", result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error fetching topic works: {str(e)}")
            return all_works
    
    async def fetch_topic_stats(self, topic_id: str) -> Optional[dict]:
        cached = get_cached_topic_stats(topic_id)
        if cached:
            return cached
        
        url = f"{OPENALEX_BASE_URL}/topics/{topic_id}"
        data = await self.make_request(url)
        
        if data:
            cache_topic_stats(topic_id, data)
        
        return data

# ============================================================================
# –°–ò–ù–•–†–û–ù–ù–´–ï –û–ë–ï–†–¢–ö–ò
# ============================================================================

def run_async(coro):
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)
    finally:
        loop.close()

def fetch_works_by_dois_sync(dois: List[str]) -> Tuple[List[dict], int, int]:
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    batches = [dois[i:i + BATCH_SIZE] for i in range(0, len(dois), BATCH_SIZE)]
    all_results = []
    successful = 0
    failed = 0
    
    async def process_batches():
        nonlocal all_results, successful, failed
        async with OpenAlexAsyncClient() as client:
            for i, batch in enumerate(batches):
                progress = (i + 1) / len(batches)
                progress_bar.progress(progress)
                status_text.text(f"Batch {i+1}/{len(batches)}: {len(batch)} DOI")
                
                results = await client.fetch_works_by_dois_batch(batch)
                
                for result in results:
                    if result:
                        successful += 1
                        all_results.append({
                            'data': result,
                            'success': True
                        })
                    else:
                        failed += 1
                        all_results.append({
                            'data': None,
                            'success': False
                        })
                
                if i < len(batches) - 1:
                    await asyncio.sleep(1)
    
    run_async(process_batches())
    
    progress_bar.empty()
    status_text.empty()
    
    return all_results, successful, failed

def fetch_works_by_topic_sync(topic_id: str, max_results: int = 2000) -> List[dict]:
    progress_bar = st.progress(0)
    status_text = st.empty()
    all_works = []
    
    def update_progress(progress, count, page):
        progress_bar.progress(progress)
        status_text.text(f"Page {page}: {count}/{max_results} works fetched")
    
    async def fetch():
        async with OpenAlexAsyncClient() as client:
            return await client.fetch_works_by_topic_cursor(
                topic_id, max_results, update_progress
            )
    
    result = run_async(fetch())
    progress_bar.empty()
    status_text.empty()
    return result

def fetch_topic_stats_sync(topic_id: str) -> Optional[dict]:
    async def fetch():
        async with OpenAlexAsyncClient() as client:
            return await client.fetch_topic_stats(topic_id)
    
    return run_async(fetch())

# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def normalize_word(word: str) -> str:
    word_lower = word.lower()
    
    if len(word_lower) < 4:
        return ''
    
    plural_exceptions = {
        'analyses': 'analysis', 'bases': 'base', 'criteria': 'criterion',
        'hypotheses': 'hypothesis', 'phenomena': 'phenomenon',
        'properties': 'property', 'activities': 'activity',
        'efficiencies': 'efficiency', 'performances': 'performance'
    }
    
    if word_lower in plural_exceptions:
        return plural_exceptions[word_lower]
    
    if word_lower.endswith('ies'):
        base = word_lower[:-3] + 'y'
        if len(base) >= 4:
            return base
    elif word_lower.endswith('es'):
        if word_lower.endswith(('ches', 'shes', 'xes', 'zes', 'sses')):
            base = word_lower[:-2]
            if len(base) >= 4:
                return base
    elif word_lower.endswith('s') and not word_lower.endswith(('ss', 'us', 'is', 'ys', 'as')):
        base = word_lower[:-1]
        if len(base) >= 4:
            return base
    
    return word_lower

def extract_keywords_from_title(title: str) -> List[str]:
    if not title:
        return []
    
    words = re.findall(r'\b[a-zA-Z]{4,}\b', title)
    filtered_words = []
    
    for word in words:
        word_lower = word.lower()
        
        if word_lower in ALL_STOPWORDS:
            continue
        
        if re.search(r'\d', word_lower):
            continue
        
        normalized = normalize_word(word_lower)
        if normalized:
            filtered_words.append(normalized)
    
    return filtered_words

def parse_doi_input(text: str) -> List[str]:
    """
    Extract DOI identifiers from text handling various formats:
    1. https://doi.org/10.1002/fuce.70042
    2. 10.1002/fuce.70042
    3. https://dx.doi.org/10.1002/fuce.70042
    4. doi:10.1002/fuce.70042
    5. DOI:10.1002/fuce.70042
    6. Full citations with doi:10.1002/cphc.201000936
    """
    if not text or not text.strip():
        return []
    
    # More comprehensive DOI pattern that handles various formats
    # Matches DOI after common prefixes and URLs
    doi_patterns = [
        r'(?i)https?://(?:dx\.)?doi\.org/(10\.\d{4,9}/[-._;()/:A-Za-z0-9]+)',  # URL format
        r'(?i)(?:doi|DOI)[:\s]+(10\.\d{4,9}/[-._;()/:A-Za-z0-9]+)',            # doi: prefix format
        r'\b(10\.\d{4,9}/[-._;()/:A-Za-z0-9]+)\b'                               # Raw DOI format
    ]
    
    all_dois = []
    
    for pattern in doi_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):  # Some patterns may return groups
                doi = match[0] if match else ''
            else:
                doi = match
            
            if doi:
                # Clean up the DOI - remove any trailing punctuation
                doi = doi.strip()
                # Remove trailing punctuation (.,;:)
                doi = re.sub(r'[.,;:]+$', '', doi)
                # Remove any angle brackets or parentheses
                doi = doi.strip('<>()[]{}')
                all_dois.append(doi)
    
    # Remove duplicates while preserving order
    seen = set()
    unique_dois = []
    for doi in all_dois:
        # Normalize DOI to lowercase for comparison
        doi_lower = doi.lower()
        if doi_lower not in seen:
            seen.add(doi_lower)
            unique_dois.append(doi)
    
    return unique_dois[:300]

def analyze_keywords_parallel(titles: List[str]) -> Counter:
    all_keywords = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(extract_keywords_from_title, title) for title in titles]
        for future in as_completed(futures):
            all_keywords.extend(future.result())
    
    return Counter(all_keywords)

def enrich_work_data(work: dict) -> dict:
    if not work:
        return {}
    
    doi_raw = work.get('doi')
    doi_clean = ''
    if doi_raw:
        doi_clean = str(doi_raw).replace('https://doi.org/', '')
    
    enriched = {
        'id': work.get('id', ''),
        'doi': doi_clean,
        'title': work.get('title', ''),
        'publication_date': work.get('publication_date', ''),
        'publication_year': work.get('publication_year', 0),
        'cited_by_count': work.get('cited_by_count', 0),
        'type': work.get('type', ''),
        'abstract': (work.get('abstract') or '')[:500],
        'doi_url': f"https://doi.org/{doi_clean}" if doi_clean else '',
    }
    
    authorships = work.get('authorships', [])
    authors = []
    institutions = set()
    
    for authorship in authorships:
        if authorship:
            author = authorship.get('author', {})
            if author:
                author_name = author.get('display_name', '')
                if author_name:
                    authors.append(author_name)
            
            for inst in authorship.get('institutions', []):
                if inst:
                    inst_name = inst.get('display_name', '')
                    if inst_name:
                        institutions.add(inst_name)
    
    enriched['authors'] = authors[:5]
    enriched['institutions'] = list(institutions)
    
    primary_location = work.get('primary_location')
    if primary_location:
        source = primary_location.get('source', {})
        enriched['journal_name'] = source.get('display_name', '') if source else ''
        enriched['journal_type'] = (source or {}).get('type', '')
    else:
        enriched['journal_name'] = ''
        enriched['journal_type'] = ''
    
    open_access = work.get('open_access')
    enriched['is_oa'] = open_access.get('is_oa', False) if open_access else False
    
    topics = work.get('topics', [])
    if topics:
        sorted_topics = sorted(topics, key=lambda x: x.get('score', 0) if x else 0, reverse=True)
        primary_topic = sorted_topics[0] if sorted_topics else {}
        enriched['primary_topic'] = primary_topic.get('display_name', '') if primary_topic else ''
        topic_id = primary_topic.get('id', '') if primary_topic else ''
        enriched['topic_id'] = topic_id.split('/')[-1] if topic_id else ''
    else:
        enriched['primary_topic'] = ''
        enriched['topic_id'] = ''
    
    return enriched

def analyze_works_for_topic(
    topic_id: str,
    keywords: List[str],
    max_citations: int = 10,
    max_works: int = 2000,
    top_n: int = 100,
    year_filter: List[int] = None,
    citation_ranges: List[Tuple[int, int]] = None
) -> List[dict]:
    
    with st.spinner(f"Loading up to {max_works} works..."):
        works = fetch_works_by_topic_sync(topic_id, max_works)
    
    if not works:
        return []
    
    current_year = datetime.now().year
    if year_filter is None:
        year_filter = [current_year - 2, current_year - 1, current_year]
    
    if citation_ranges is None:
        citation_ranges = [(0, 10)]
    
    with st.spinner(f"Analyzing {len(works)} works..."):
        analyzed = []
        
        for work in works:
            cited_by_count = work.get('cited_by_count', 0)
            publication_year = work.get('publication_year', 0)
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –≥–æ–¥–∞–º
            if publication_year not in year_filter:
                continue
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º (–¥–∏–∞–ø–∞–∑–æ–Ω—ã)
            in_range = False
            for min_cit, max_cit in citation_ranges:
                if min_cit <= cited_by_count <= max_cit:
                    in_range = True
                    break
            
            if not in_range:
                continue
            
            title = work.get('title', '')
            abstract = work.get('abstract', '')
            
            if title:
                title_lower = title.lower()
                abstract_lower = abstract.lower() if abstract else ''
                
                score = 0
                matched = []
                
                for keyword in keywords:
                    kw_lower = keyword.lower()
                    if kw_lower in title_lower:
                        score += 3
                        matched.append(keyword)
                    elif abstract and kw_lower in abstract_lower:
                        score += 1
                        matched.append(f"{keyword}*")
                
                if score > 0:
                    enriched = enrich_work_data(work)
                    enriched.update({
                        'relevance_score': score,
                        'matched_keywords': matched,
                        'analysis_time': datetime.now().isoformat()
                    })
                    analyzed.append(enriched)
        
        analyzed.sort(key=lambda x: x['relevance_score'], reverse=True)
        return analyzed[:top_n]

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –≠–ö–°–ü–û–†–¢–ê
# ============================================================================

def generate_csv(data: List[dict], metadata: Dict = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è CSV —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    df = pd.DataFrame(data)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
    csv_content = []
    
    if metadata:
        csv_content.append("# " + "=" * 70)
        csv_content.append("# CTA Article Recommender Pro - Analysis Report")
        csv_content.append("# " + "=" * 70)
        csv_content.append(f"# Generated: {metadata.get('generated_date', '')}")
        csv_content.append(f"# Research Topic: {metadata.get('topic_name', '')}")
        csv_content.append(f"# Total analyzed papers: {metadata.get('total_papers', len(data))}")
        csv_content.append(f"# Analysis parameters:")
        
        if metadata.get('original_dois'):
            dois = metadata['original_dois']
            csv_content.append(f"# - Input DOIs: {len(dois)} identifiers")
            if len(dois) <= 5:
                for doi in dois:
                    csv_content.append(f"#   ‚Ä¢ {doi}")
            else:
                csv_content.append(f"#   ‚Ä¢ First 3: {dois[0]}, {dois[1]}, {dois[2]}...")
                csv_content.append(f"#   ‚Ä¢ Last 3: ...{dois[-3]}, {dois[-2]}, {dois[-1]}")
        
        if metadata.get('analysis_filters'):
            filters = metadata['analysis_filters']
            csv_content.append(f"# - Publication years: {filters.get('years', 'All')}")
            csv_content.append(f"# - Citation ranges: {filters.get('citation_ranges', '0-10')}")
            csv_content.append(f"# - Max citations: {filters.get('max_citations', 10)}")
        
        if metadata.get('keywords_used'):
            keywords = metadata['keywords_used']
            csv_content.append(f"# - Keywords used for relevance: {', '.join(keywords[:10])}")
            if len(keywords) > 10:
                csv_content.append(f"#   ... and {len(keywords)-10} more")
        
        csv_content.append("# " + "-" * 70)
        csv_content.append("#")
        csv_content.append("# To reproduce this analysis:")
        csv_content.append("# 1. Use the same DOIs as input")
        csv_content.append("# 2. Apply the same filters")
        csv_content.append("# 3. Run CTA Article Recommender Pro")
        csv_content.append("# " + "=" * 70)
        csv_content.append("")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    csv_content.append(df.to_csv(index=False, encoding='utf-8-sig'))
    
    return "\n".join(csv_content)

def generate_excel(data: List[dict], metadata: Dict = None) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    output = io.BytesIO()
    
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book
        
        # ========== –í–ö–õ–ê–î–ö–ê –° –ú–ï–¢–ê–î–ê–ù–ù–´–ú–ò ==========
        if metadata:
            metadata_sheet = workbook.add_worksheet('Analysis Info')
            
            # –§–æ—Ä–º–∞—Ç—ã
            title_format = workbook.add_format({
                'bold': True,
                'font_size': 14,
                'font_color': '#2C3E50',
                'align': 'center',
                'valign': 'vcenter',
                'border': 1,
                'bg_color': '#ECF0F1'
            })
            
            header_format = workbook.add_format({
                'bold': True,
                'font_size': 11,
                'font_color': 'white',
                'bg_color': '#3498DB',
                'border': 1,
                'align': 'left',
                'valign': 'vcenter'
            })
            
            data_format = workbook.add_format({
                'font_size': 10,
                'align': 'left',
                'valign': 'vcenter',
                'border': 1,
                'text_wrap': True
            })
            
            doi_format = workbook.add_format({
                'font_size': 9,
                'font_color': '#2980B9',
                'align': 'left',
                'valign': 'vcenter',
                'font_name': 'Courier New'
            })
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            metadata_sheet.merge_range('A1:F1', 'CTA Article Recommender Pro - Analysis Report', title_format)
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            row = 2
            metadata_sheet.write(row, 0, 'Generated:', header_format)
            metadata_sheet.write(row, 1, metadata.get('generated_date', ''), data_format)
            row += 1
            
            metadata_sheet.write(row, 0, 'Research Topic:', header_format)
            metadata_sheet.write(row, 1, metadata.get('topic_name', ''), data_format)
            row += 1
            
            metadata_sheet.write(row, 0, 'Total papers found:', header_format)
            metadata_sheet.write(row, 1, len(data), data_format)
            row += 2
            
            # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            metadata_sheet.write(row, 0, 'INPUT DATA', workbook.add_format({
                'bold': True,
                'font_size': 12,
                'font_color': '#2C3E50',
                'bg_color': '#BDC3C7',
                'border': 1
            }))
            metadata_sheet.merge_range(row, 1, row, 5, '', header_format)
            row += 1
            
            if metadata.get('original_dois'):
                dois = metadata['original_dois']
                metadata_sheet.write(row, 0, 'Original DOIs:', header_format)
                metadata_sheet.write(row, 1, f'{len(dois)} identifiers', data_format)
                row += 1
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20 DOI
                metadata_sheet.write(row, 0, 'DOI List:', header_format)
                row += 1
                
                for i, doi in enumerate(dois[:20]):
                    metadata_sheet.write(row + i, 0, f'{i+1}.', data_format)
                    metadata_sheet.write(row + i, 1, doi, doi_format)
                
                row += min(20, len(dois)) + 1
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
            metadata_sheet.write(row, 0, 'ANALYSIS PARAMETERS', workbook.add_format({
                'bold': True,
                'font_size': 12,
                'font_color': '#2C3E50',
                'bg_color': '#BDC3C7',
                'border': 1
            }))
            metadata_sheet.merge_range(row, 1, row, 5, '', header_format)
            row += 1
            
            if metadata.get('analysis_filters'):
                filters = metadata['analysis_filters']
                metadata_sheet.write(row, 0, 'Publication years:', header_format)
                metadata_sheet.write(row, 1, ', '.join(map(str, filters.get('years', []))), data_format)
                row += 1
                
                metadata_sheet.write(row, 0, 'Citation ranges:', header_format)
                metadata_sheet.write(row, 1, filters.get('citation_ranges_display', '0-10'), data_format)
                row += 1
                
                metadata_sheet.write(row, 0, 'Max citations:', header_format)
                metadata_sheet.write(row, 1, filters.get('max_citations', 10), data_format)
                row += 2
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if metadata.get('keywords_used'):
                keywords = metadata['keywords_used']
                metadata_sheet.write(row, 0, 'KEYWORDS USED', workbook.add_format({
                    'bold': True,
                    'font_size': 12,
                    'font_color': '#2C3E50',
                    'bg_color': '#BDC3C7',
                    'border': 1
                }))
                metadata_sheet.merge_range(row, 1, row, 5, '', header_format)
                row += 1
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ 5
                for i in range(0, len(keywords), 5):
                    chunk = keywords[i:i+5]
                    metadata_sheet.write(row, 0, f'Group {i//5 + 1}:', header_format)
                    metadata_sheet.write(row, 1, ', '.join(chunk), data_format)
                    row += 1
            
            # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é
            row += 1
            metadata_sheet.write(row, 0, 'HOW TO REPRODUCE', workbook.add_format({
                'bold': True,
                'font_size': 11,
                'font_color': '#27AE60',
                'bg_color': '#D5F4E6',
                'border': 1
            }))
            metadata_sheet.merge_range(row, 1, row, 5, '', header_format)
            row += 1
            
            reproduce_steps = [
                "1. Use the same DOI identifiers as input",
                "2. Select the same research topic",
                "3. Apply identical filters:",
                "   - Same publication years",
                "   - Same citation ranges",
                "   - Same keyword set",
                "4. Run analysis in CTA Article Recommender Pro",
                "",
                "Note: Results may vary slightly due to data updates in OpenAlex"
            ]
            
            for i, step in enumerate(reproduce_steps):
                metadata_sheet.write(row + i, 0, step, data_format)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫
            metadata_sheet.set_column('A:A', 25)
            metadata_sheet.set_column('B:F', 40)
        
        # ========== –í–ö–õ–ê–î–ö–ê –° –î–ê–ù–ù–´–ú–ò ==========
        df = pd.DataFrame(data)
        df.to_excel(writer, sheet_name='Papers', index=False)
        
        worksheet = writer.sheets['Papers']
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        header_format = workbook.add_format({
            'bold': True,
            'font_size': 11,
            'font_color': 'white',
            'bg_color': '#667eea',
            'border': 1,
            'align': 'center',
            'valign': 'vcenter'
        })
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        for col_num, value in enumerate(df.columns.values):
            worksheet.write(0, col_num, value, header_format)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        data_format = workbook.add_format({
            'font_size': 9,
            'border': 1,
            'text_wrap': True,
            'valign': 'top'
        })
        
        citation_format = workbook.add_format({
            'font_size': 9,
            'border': 1,
            'align': 'center',
            'bold': True,
            'font_color': '#E74C3C'
        })
        
        relevance_format = workbook.add_format({
            'font_size': 9,
            'border': 1,
            'align': 'center',
            'bg_color': '#D5F4E6'
        })
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç—ã –∫ –¥–∞–Ω–Ω—ã–º
        for row_num in range(1, len(df) + 1):
            for col_num in range(len(df.columns)):
                cell_value = df.iat[row_num-1, col_num]
                col_name = df.columns[col_num]
                
                if col_name in ['cited_by_count', 'Citations']:
                    worksheet.write(row_num, col_num, cell_value, citation_format)
                elif col_name in ['relevance_score', 'Relevance']:
                    worksheet.write(row_num, col_num, cell_value, relevance_format)
                else:
                    worksheet.write(row_num, col_num, cell_value, data_format)
        
        # –ê–≤—Ç–æ-—à–∏—Ä–∏–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
        for i, col in enumerate(df.columns):
            column_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
            worksheet.set_column(i, i, min(column_len, 50))
    
    return output.getvalue()

def generate_pdf(data: List[dict], topic_name: str, metadata: Dict = None) -> bytes:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF —Ñ–∞–π–ª–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º, –∞–∫—Ç–∏–≤–Ω—ã–º–∏ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    
    buffer = io.BytesIO()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º A4 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    doc = SimpleDocTemplate(
        buffer, 
        pagesize=A4,
        topMargin=1*cm,
        bottomMargin=1*cm,
        leftMargin=1.5*cm,
        rightMargin=1.5*cm
    )
    
    styles = getSampleStyleSheet()
    
    # ========== –°–û–ó–î–ê–ù–ò–ï –ö–ê–°–¢–û–ú–ù–´–• –°–¢–ò–õ–ï–ô ==========
    
    # –°—Ç–∏–ª—å –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=18,
        textColor=colors.HexColor('#2C3E50'),
        spaceAfter=12,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Heading2'],
        fontSize=14,
        textColor=colors.HexColor('#34495E'),
        spaceAfter=8,
        alignment=TA_CENTER,
        fontName='Helvetica'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–º–µ
    topic_style = ParagraphStyle(
        'CustomTopic',
        parent=styles['Heading3'],
        fontSize=12,
        textColor=colors.HexColor('#16A085'),
        spaceAfter=6,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    meta_style = ParagraphStyle(
        'CustomMeta',
        parent=styles['Normal'],
        fontSize=10,
        textColor=colors.HexColor('#7F8C8D'),
        spaceAfter=3,
        alignment=TA_CENTER,
        fontName='Helvetica-Oblique'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏ (—Å –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–æ–π)
    paper_title_style = ParagraphStyle(
        'CustomPaperTitle',
        parent=styles['Heading4'],
        fontSize=11,
        textColor=colors.HexColor('#2980B9'),
        spaceAfter=4,
        alignment=TA_LEFT,
        fontName='Helvetica-Bold',
        underline=True
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –∞–≤—Ç–æ—Ä–æ–≤
    authors_style = ParagraphStyle(
        'CustomAuthors',
        parent=styles['Normal'],
        fontSize=9,
        textColor=colors.HexColor('#2C3E50'),
        spaceAfter=2,
        alignment=TA_LEFT,
        fontName='Helvetica'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –¥–µ—Ç–∞–ª–µ–π —Å—Ç–∞—Ç—å–∏
    details_style = ParagraphStyle(
        'CustomDetails',
        parent=styles['Normal'],
        fontSize=8,
        textColor=colors.HexColor('#7F8C8D'),
        spaceAfter=2,
        alignment=TA_LEFT,
        fontName='Helvetica'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫
    metrics_style = ParagraphStyle(
        'CustomMetrics',
        parent=styles['Normal'],
        fontSize=9,
        textColor=colors.HexColor('#27AE60'),
        spaceAfter=0,
        alignment=TA_LEFT,
        fontName='Helvetica-Bold'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    keywords_style = ParagraphStyle(
        'CustomKeywords',
        parent=styles['Normal'],
        fontSize=8,
        textColor=colors.HexColor('#E74C3C'),
        spaceAfter=2,
        alignment=TA_LEFT,
        fontName='Helvetica-Oblique'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –Ω–∏–∂–Ω–µ–≥–æ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–∞
    footer_style = ParagraphStyle(
        'CustomFooter',
        parent=styles['Normal'],
        fontSize=8,
        textColor=colors.HexColor('#95A5A6'),
        spaceBefore=15,
        alignment=TA_CENTER,
        fontName='Helvetica-Oblique'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
    separator_style = ParagraphStyle(
        'CustomSeparator',
        parent=styles['Normal'],
        fontSize=8,
        textColor=colors.HexColor('#BDC3C7'),
        spaceAfter=10,
        spaceBefore=10,
        alignment=TA_CENTER,
        fontName='Helvetica-Oblique'
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞–Ω–Ω—ã—Ö
    data_info_style = ParagraphStyle(
        'CustomDataInfo',
        parent=styles['Normal'],
        fontSize=9,
        textColor=colors.HexColor('#8E44AD'),
        spaceAfter=3,
        alignment=TA_LEFT,
        fontName='Helvetica',
        backColor=colors.HexColor('#F4ECF7'),
        borderPadding=5,
        borderColor=colors.HexColor('#D2B4DE'),
        borderWidth=1
    )
    
    # –°—Ç–∏–ª—å –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    params_style = ParagraphStyle(
        'CustomParams',
        parent=styles['Normal'],
        fontSize=9,
        textColor=colors.HexColor('#2C3E50'),
        spaceAfter=3,
        alignment=TA_LEFT,
        fontName='Helvetica',
        leftIndent=10
    )
    
    story = []
    
    # ========== –ó–ê–ì–û–õ–û–í–û–ß–ù–ê–Ø –°–¢–†–ê–ù–ò–¶–ê ==========
    
    story.append(Spacer(1, 2*cm))
    story.append(Paragraph("CTA Article Recommender Pro", title_style))
    story.append(Paragraph("Fresh Papers Analysis Report", subtitle_style))
    story.append(Spacer(1, 1*cm))
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–º–µ
    story.append(Paragraph(f"RESEARCH TOPIC:", topic_style))
    story.append(Paragraph(f"{topic_name.upper()}", subtitle_style))
    story.append(Spacer(1, 0.5*cm))
    
    # –ú–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    current_date = datetime.now().strftime('%B %d, %Y at %H:%M')
    story.append(Paragraph(f"Generated on {current_date}", meta_style))
    story.append(Paragraph(f"Total papers analyzed: {len(data)}", meta_style))
    
    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    if data:
        avg_citations = np.mean([w.get('cited_by_count', 0) for w in data])
        oa_count = sum(1 for w in data if w.get('is_oa'))
        recent_count = sum(1 for w in data if w.get('publication_year', 0) >= datetime.now().year - 2)
        
        stats_text = f"""
        Average citations: {avg_citations:.1f} | 
        Open Access papers: {oa_count} | 
        Recent papers (‚â§2 years): {recent_count}
        """
        story.append(Paragraph(stats_text, meta_style))
    
    story.append(Spacer(1, 1.5*cm))
    
    # –ö–æ–ø–∏—Ä–∞–π—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    story.append(Paragraph("¬© CTA - Chimica Techno Acta", footer_style))
    story.append(Paragraph("https://chimicatechnoacta.ru", footer_style))
    story.append(Paragraph("Developed by daM¬©", footer_style))
    
    # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å—Ç—Ä–∞–Ω–∏—Ü
    story.append(PageBreak())
    
    # ========== –°–¢–†–ê–ù–ò–¶–ê –° –ú–ï–¢–ê–î–ê–ù–ù–´–ú–ò –ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò ==========
    
    story.append(Paragraph("ANALYSIS METADATA & PARAMETERS", title_style))
    story.append(Spacer(1, 0.5*cm))
    
    # –ë–ª–æ–∫ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if metadata and metadata.get('original_dois'):
        dois = metadata['original_dois']
        
        story.append(Paragraph("ORIGINAL INPUT DATA", ParagraphStyle(
            'DataHeader',
            parent=styles['Heading3'],
            fontSize=12,
            textColor=colors.HexColor('#8E44AD'),
            spaceAfter=8,
            alignment=TA_LEFT,
            fontName='Helvetica-Bold'
        )))
        
        story.append(Paragraph(f"<b>Number of DOI identifiers analyzed:</b> {len(dois)}", data_info_style))
        
        if len(dois) <= 10:
            doi_list = "<br/>".join([f"‚Ä¢ {doi}" for doi in dois])
        else:
            first_five = "<br/>".join([f"‚Ä¢ {doi}" for doi in dois[:5]])
            last_five = "<br/>".join([f"‚Ä¢ {doi}" for doi in dois[-5:]])
            doi_list = f"{first_five}<br/>...<br/>{last_five}"
        
        story.append(Paragraph(f"<b>DOI identifiers:</b><br/>{doi_list}", data_info_style))
        story.append(Spacer(1, 0.5*cm))
    
    # –ë–ª–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    if metadata and metadata.get('analysis_filters'):
        filters = metadata['analysis_filters']
        
        story.append(Paragraph("ANALYSIS PARAMETERS", ParagraphStyle(
            'ParamsHeader',
            parent=styles['Heading3'],
            fontSize=12,
            textColor=colors.HexColor('#16A085'),
            spaceAfter=8,
            alignment=TA_LEFT,
            fontName='Helvetica-Bold'
        )))
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        params_data = [
            ["Parameter", "Value"],
            ["Publication years", ', '.join(map(str, filters.get('years', [])))],
            ["Citation ranges", filters.get('citation_ranges_display', '0-10')],
            ["Maximum citations", str(filters.get('max_citations', 10))],
            ["Papers per topic", str(filters.get('max_works', 2000))],
            ["Top papers limit", str(filters.get('top_n', 100))]
        ]
        
        params_table = Table(params_data, colWidths=[doc.width/3, doc.width*2/3])
        params_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#16A085')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
            ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 8),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#D5DBDB')),
            ('FONTSIZE', (0, 1), (-1, -1), 9),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#F2F4F4')]),
        ]))
        
        story.append(params_table)
        story.append(Spacer(1, 0.5*cm))
    
    # –ë–ª–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    if metadata and metadata.get('keywords_used'):
        keywords = metadata['keywords_used']
        
        story.append(Paragraph("KEYWORDS FOR RELEVANCE SCORING", ParagraphStyle(
            'KeywordsHeader',
            parent=styles['Heading3'],
            fontSize=12,
            textColor=colors.HexColor('#E74C3C'),
            spaceAfter=8,
            alignment=TA_LEFT,
            fontName='Helvetica-Bold'
        )))
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –∫–æ–ª–æ–Ω–∫–∏
        keywords_text = ""
        for i, keyword in enumerate(keywords[:20], 1):
            keywords_text += f"‚Ä¢ {keyword} "
            if i % 5 == 0:
                keywords_text += "<br/>"
        
        story.append(Paragraph(keywords_text, keywords_style))
        story.append(Spacer(1, 0.5*cm))
    
    # –ë–ª–æ–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
    story.append(Paragraph("HOW TO REPRODUCE THESE RESULTS", ParagraphStyle(
        'ReproduceHeader',
        parent=styles['Heading3'],
        fontSize=12,
        textColor=colors.HexColor('#3498DB'),
        spaceAfter=8,
        alignment=TA_LEFT,
        fontName='Helvetica-Bold'
    )))
    
    reproduce_text = """
    1. <b>Input Data:</b> Use the same DOI identifiers as listed above<br/>
    2. <b>Topic Selection:</b> Choose the same research topic<br/>
    3. <b>Filters:</b> Apply identical analysis parameters<br/>
    4. <b>Analysis:</b> Run the analysis in CTA Article Recommender Pro<br/>
    <br/>
    <i>Note: Results may vary slightly due to data updates in OpenAlex database.</i>
    """
    
    story.append(Paragraph(reproduce_text, params_style))
    story.append(Spacer(1, 1*cm))
    
    # QR –∫–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ)
    # story.append(Paragraph("Scan to access the tool:", details_style))
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å QR –∫–æ–¥, –µ—Å–ª–∏ –µ—Å—Ç—å URL
    
    story.append(PageBreak())
    
    # ========== –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï ==========
    
    story.append(Paragraph("TABLE OF CONTENTS", title_style))
    story.append(Spacer(1, 0.5*cm))
    
    # –°–æ–∑–¥–∞–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
    toc_items = []
    for i in range(min(50, len(data))):
        title = data[i].get('title', 'Untitled')
        title_clean = re.sub(r'<[^>]+>', '', title)
        title_clean = title_clean.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        toc_items.append(f"{i+1}. {title_clean[:80]}...")
    
    toc_text = "<br/>".join(toc_items[:20])
    story.append(Paragraph(toc_text, details_style))
    
    if len(data) > 20:
        story.append(Paragraph(f"... and {len(data)-20} more papers", details_style))
    
    story.append(PageBreak())
    
    # ========== –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –°–¢–ê–¢–¨–Ø–ú ==========
    
    story.append(Paragraph("DETAILED PAPER ANALYSIS", title_style))
    story.append(Spacer(1, 0.5*cm))
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 50 —Å—Ç–∞—Ç—å—è–º–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    display_data = data[:50]
    
    for i, work in enumerate(display_data, 1):
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ —Å –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–æ–π
        title = work.get('title', 'No title available')
        doi = work.get('doi', '')
        doi_url = work.get('doi_url', '')
        
        # –°–æ–∑–¥–∞–µ–º –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫—É –≤ PDF
        if doi_url:
            title_with_link = f'<link href="{doi_url}" color="blue"><u>{title}</u></link>'
        else:
            title_with_link = title
        
        story.append(Paragraph(f"{i}. {title_with_link}", paper_title_style))
        
        # –ê–≤—Ç–æ—Ä—ã
        authors = work.get('authors', [])
        if authors:
            authors_text = ', '.join(authors[:3])
            if len(authors) > 3:
                authors_text += f' et al. ({len(authors)} authors)'
            story.append(Paragraph(f"<b>Authors:</b> {authors_text}", authors_style))
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
        citations = work.get('cited_by_count', 0)
        year = work.get('publication_year', 'N/A')
        relevance = work.get('relevance_score', 0)
        journal = work.get('journal_name', 'N/A')[:40]
        
        metrics_text = f"""
        <b>Citations:</b> {citations} | 
        <b>Year:</b> {year} | 
        <b>Relevance Score:</b> {relevance}/10 | 
        <b>Journal:</b> {journal} | 
        <b>Open Access:</b> {'Yes' if work.get('is_oa') else 'No'}
        """
        story.append(Paragraph(metrics_text, metrics_style))
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if work.get('matched_keywords'):
            keywords = ', '.join(work.get('matched_keywords', [])[:5])
            story.append(Paragraph(f"<b>Matched Keywords:</b> {keywords}", keywords_style))
        
        # DOI —Å—Å—ã–ª–∫–∞
        if doi:
            if doi_url:
                doi_link = f'<link href="{doi_url}" color="blue"><u>{doi}</u></link>'
            else:
                doi_link = doi
            story.append(Paragraph(f"<b>DOI:</b> {doi_link}", details_style))
        
        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
        if i < len(display_data):
            story.append(Paragraph("‚îÄ" * 80, separator_style))
            story.append(Spacer(1, 0.2*cm))
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –µ—â–µ —Å—Ç–∞—Ç—å–∏, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    if len(data) > 50:
        story.append(Spacer(1, 0.5*cm))
        story.append(Paragraph(f"Note: Showing 50 out of {len(data)} papers. "
                              f"Full list available in CSV/Excel export.", 
                             ParagraphStyle(
                                 'NoteStyle',
                                 parent=styles['Normal'],
                                 fontSize=9,
                                 textColor=colors.HexColor('#7F8C8D'),
                                 alignment=TA_CENTER,
                                 fontName='Helvetica-Oblique'
                             )))
    
    # ========== –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –°–¢–†–ê–ù–ò–¶–ê ==========
    
    if len(data) > 10:
        story.append(PageBreak())
        story.append(Paragraph("STATISTICAL SUMMARY", title_style))
        story.append(Spacer(1, 0.5*cm))
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        citations_list = [w.get('cited_by_count', 0) for w in data]
        years_list = [w.get('publication_year', 0) for w in data if w.get('publication_year', 0) > 1900]
        
        if citations_list and years_list:
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_data = [
                ["Metric", "Value"],
                ["Total Papers", len(data)],
                ["Average Citations", f"{np.mean(citations_list):.2f}"],
                ["Median Citations", f"{np.median(citations_list):.2f}"],
                ["Min Citations", min(citations_list)],
                ["Max Citations", max(citations_list)],
                ["Open Access Papers", sum(1 for w in data if w.get('is_oa'))],
                ["Average Year", f"{np.mean(years_list):.1f}"],
                ["Most Recent Year", max(years_list) if years_list else "N/A"],
                ["Average Relevance", f"{np.mean([w.get('relevance_score', 0) for w in data]):.2f}/10"]
            ]
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats_table = Table(stats_data, colWidths=[doc.width/2.5, doc.width/3])
            stats_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498DB')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 11),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#F8F9FA')),
                ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#D5DBDB')),
                ('FONTSIZE', (0, 1), (-1, -1), 10),
                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#F2F4F4')]),
            ]))
            
            story.append(stats_table)
            story.append(Spacer(1, 1*cm))
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
            if years_list:
                year_counts = {}
                for year in years_list:
                    year_counts[year] = year_counts.get(year, 0) + 1
                
                sorted_years = sorted(year_counts.items())
                year_data = [["Year", "Number of Papers"]] + [[str(y), str(c)] for y, c in sorted_years[-10:]]
                
                if len(year_data) > 1:
                    story.append(Paragraph("Publications by Year (Last 10 years)", subtitle_style))
                    year_table = Table(year_data, colWidths=[doc.width/4, doc.width/4])
                    year_table.setStyle(TableStyle([
                        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2ECC71')),
                        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#D5DBDB')),
                        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#F2F4F4')]),
                    ]))
                    story.append(year_table)
    
    # ========== –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï ==========
    
    story.append(PageBreak())
    story.append(Paragraph("CONCLUSION & RECOMMENDATIONS", title_style))
    story.append(Spacer(1, 0.5*cm))
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
    conclusions = [
        f"This report analyzed {len(data)} fresh papers in the field of '{topic_name}'.",
        "Papers with low citation counts often represent emerging ideas or niche research areas.",
        "Consider these papers for:",
        "‚Ä¢ Literature reviews of emerging topics",
        "‚Ä¢ Identifying research gaps",
        "‚Ä¢ Finding novel methodologies",
        "‚Ä¢ Cross-disciplinary connections"
    ]
    
    for conclusion in conclusions:
        story.append(Paragraph(conclusion, ParagraphStyle(
            'Conclusion',
            parent=styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#2C3E50'),
            spaceAfter=4,
            leftIndent=20 if conclusion.startswith('‚Ä¢') else 0,
            fontName='Helvetica'
        )))
    
    story.append(Spacer(1, 1*cm))
    
    # –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è
    story.append(Paragraph("FINAL NOTES", subtitle_style))
    final_notes = [
        "This report was generated automatically by CTA Article Recommender Pro.",
        "All data is sourced from OpenAlex API and is subject to their terms of use.",
        "For the most current data, please visit the original sources via the provided DOIs.",
        "Citation counts are as of the report generation date and may change over time."
    ]
    
    for note in final_notes:
        story.append(Paragraph(f"‚Ä¢ {note}", details_style))
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    story.append(Spacer(1, 0.5*cm))
    story.append(Paragraph("REPRODUCIBILITY INFORMATION", subtitle_style))
    reprod_info = [
        f"Report generated on: {current_date}",
        f"Analysis parameters saved in metadata section",
        f"Use identical inputs and filters to reproduce results",
        f"Report ID: {hashlib.md5(str(datetime.now()).encode()).hexdigest()[:12].upper()}"
    ]
    
    for info in reprod_info:
        story.append(Paragraph(f"‚Ä¢ {info}", details_style))
    
    # –ù–∏–∂–Ω–∏–π –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    story.append(Spacer(1, 2*cm))
    story.append(Paragraph("¬© CTA Article Recommender Pro - https://chimicatechnoacta.ru", footer_style))
    story.append(Paragraph(f"Report ID: {hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]}", 
                         ParagraphStyle(
                             'ReportID',
                             parent=styles['Normal'],
                             fontSize=7,
                             textColor=colors.HexColor('#BDC3C7'),
                             alignment=TA_CENTER
                         )))
    
    # ========== –ì–ï–ù–ï–†–ê–¶–ò–Ø PDF ==========
    
    doc.build(story)
    
    return buffer.getvalue()

def generate_txt(data: List[dict], topic_name: str, metadata: Dict = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è TXT —Ñ–∞–π–ª–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ —Ü–≤–µ—Ç–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏"""
    
    output = []
    
    # ANSI —Ü–≤–µ—Ç–æ–≤—ã–µ –∫–æ–¥—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö —Ü–≤–µ—Ç–∞
    # –ï—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ü–≤–µ—Ç–∞, –æ–Ω–∏ –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
    class Colors:
        HEADER = '\033[95m'
        BLUE = '\033[94m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        RED = '\033[91m'
        PURPLE = '\033[95m'
        CYAN = '\033[96m'
        WHITE = '\033[97m'
        BOLD = '\033[1m'
        UNDERLINE = '\033[4m'
        END = '\033[0m'
        BG_BLUE = '\033[44m'
        BG_GREEN = '\033[42m'
        BG_YELLOW = '\033[43m'
        BG_RED = '\033[41m'
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª —Ü–≤–µ—Ç–∞ (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è —Ñ–∞–π–ª–∞)
    use_colors = True  # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ–ø—Ü–∏–µ–π
    
    def colorize(text, color_code):
        return f"{color_code}{text}{Colors.END}" if use_colors else text
    
    # ========== –ó–ê–ì–û–õ–û–í–û–ö –° –¶–í–ï–¢–ù–´–ú –ë–ê–ù–ù–ï–†–û–ú ==========
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append(colorize("  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ", Colors.BLUE))
    output.append(colorize(" ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù  ", Colors.BLUE))
    output.append(colorize(" ‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ", Colors.BLUE))
    output.append(colorize(" ‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù    ", Colors.BLUE))
    output.append(colorize(" ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ", Colors.BLUE))
    output.append(colorize("  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ", Colors.BLUE))
    output.append("")
    output.append(colorize("                    ARTICLE RECOMMENDER PRO - ANALYSIS REPORT", Colors.BOLD + Colors.WHITE))
    output.append(colorize("                    Fresh Papers Analysis for Research Discovery", Colors.YELLOW))
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    # ========== –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –¢–ï–ú–ï ==========
    output.append(colorize("üìö RESEARCH TOPIC:", Colors.BOLD + Colors.PURPLE))
    output.append(colorize(f"  {'‚ïê' * 50}", Colors.PURPLE))
    output.append(colorize(f"  {topic_name.upper()}", Colors.BOLD + Colors.WHITE))
    output.append(colorize(f"  {'‚ïê' * 50}", Colors.PURPLE))
    output.append("")
    
    # ========== –ú–ï–¢–ê-–ò–ù–§–û–†–ú–ê–¶–ò–Ø –° –¶–í–ï–¢–ù–´–ú–ò –ò–ö–û–ù–ö–ê–ú–ò ==========
    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    output.append(colorize("üìã REPORT INFORMATION:", Colors.BOLD + Colors.CYAN))
    output.append(f"  {colorize('üïí', Colors.YELLOW)} Generated: {colorize(current_date, Colors.GREEN)}")
    output.append(f"  {colorize('üìÑ', Colors.BLUE)} Papers analyzed: {colorize(str(len(data)), Colors.GREEN)}")
    
    if data:
        avg_citations = np.mean([w.get('cited_by_count', 0) for w in data])
        oa_count = sum(1 for w in data if w.get('is_oa'))
        recent_count = sum(1 for w in data if w.get('publication_year', 0) >= datetime.now().year - 2)
        
        output.append(f"  {colorize('üìä', Colors.PURPLE)} Average citations: {colorize(f'{avg_citations:.2f}', Colors.GREEN)}")
        output.append(f"  {colorize('üîì', Colors.GREEN)} Open Access papers: {colorize(str(oa_count), Colors.GREEN)}")
        output.append(f"  {colorize('üîÑ', Colors.BLUE)} Recent papers (‚â§2 years): {colorize(str(recent_count), Colors.GREEN)}")
    
    output.append("")
    
    # ========== –ú–ï–¢–ê–î–ê–ù–ù–´–ï –û–ë –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–• ==========
    if metadata:
        output.append(colorize("üîç INPUT DATA & PARAMETERS:", Colors.BOLD + Colors.PURPLE))
        output.append(colorize("  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ", Colors.PURPLE))
        
        if metadata.get('original_dois'):
            dois = metadata['original_dois']
            output.append(f"  {colorize('üî¢', Colors.CYAN)} Original DOI identifiers: {colorize(str(len(dois)), Colors.GREEN)}")
            output.append(f"  {colorize('üìù', Colors.YELLOW)} Sample DOIs analyzed:")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 DOI
            for i, doi in enumerate(dois[:3]):
                output.append(f"     {i+1}. {colorize(doi, Colors.BLUE)}")
            
            if len(dois) > 3:
                output.append(f"     ... and {len(dois)-3} more")
                if len(dois) > 6:
                    output.append(f"     Last 3: {colorize(dois[-3], Colors.BLUE)}, {colorize(dois[-2], Colors.BLUE)}, {colorize(dois[-1], Colors.BLUE)}")
        
        if metadata.get('analysis_filters'):
            filters = metadata['analysis_filters']
            output.append(f"  {colorize('‚öôÔ∏è', Colors.YELLOW)} Analysis parameters:")
            output.append(f"     ‚Ä¢ {colorize('Publication years:', Colors.WHITE)} {colorize(', '.join(map(str, filters.get('years', []))), Colors.GREEN)}")
            output.append(f"     ‚Ä¢ {colorize('Citation ranges:', Colors.WHITE)} {colorize(filters.get('citation_ranges_display', '0-10'), Colors.GREEN)}")
            output.append(f"     ‚Ä¢ {colorize('Max citations:', Colors.WHITE)} {colorize(str(filters.get('max_citations', 10)), Colors.GREEN)}")
        
        if metadata.get('keywords_used'):
            keywords = metadata['keywords_used']
            output.append(f"  {colorize('üè∑Ô∏è', Colors.RED)} Keywords for relevance scoring:")
            keywords_line = ', '.join(keywords[:10])
            output.append(f"     {colorize(keywords_line, Colors.YELLOW)}")
            if len(keywords) > 10:
                output.append(f"     ... and {len(keywords)-10} more")
        
        output.append("")
    
    # ========== –ö–ê–ö –í–û–°–ü–†–û–ò–ó–í–ï–°–¢–ò –†–ï–ó–£–õ–¨–¢–ê–¢–´ ==========
    output.append(colorize("üîÑ HOW TO REPRODUCE:", Colors.BOLD + Colors.GREEN))
    output.append(colorize("  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ", Colors.GREEN))
    reproduce_steps = [
        f"1. {colorize('Use the same DOI identifiers', Colors.WHITE)} as listed above",
        f"2. {colorize('Select the same research topic', Colors.WHITE)} in the tool",
        f"3. {colorize('Apply identical analysis filters:', Colors.WHITE)}",
        f"   ‚Ä¢ Same publication years",
        f"   ‚Ä¢ Same citation ranges",
        f"   ‚Ä¢ Same maximum citations limit",
        f"4. {colorize('Run analysis in CTA Article Recommender Pro', Colors.WHITE)}",
        "",
        f"{colorize('Note:', Colors.YELLOW)} Results may vary slightly due to data updates in OpenAlex database."
    ]
    output.extend(reproduce_steps)
    output.append("")
    
    # ========== –ö–û–ü–ò–†–ê–ô–¢ –ò –ò–ù–§–û–†–ú–ê–¶–ò–Ø ==========
    output.append(colorize("¬© CTA - Chimica Techno Acta", Colors.CYAN))
    output.append(colorize("üåê https://chimicatechnoacta.ru", Colors.BLUE))
    output.append(colorize("üë®‚Äçüíª Developed by daM¬©", Colors.PURPLE))
    output.append("")
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    # ========== –û–ì–õ–ê–í–õ–ï–ù–ò–ï –° –¶–í–ï–¢–ù–´–ú–ò –†–ê–ó–î–ï–õ–ê–ú–ò ==========
    output.append(colorize("üìë TABLE OF CONTENTS", Colors.BOLD + Colors.PURPLE))
    output.append(colorize("‚ïê" * 50, Colors.PURPLE))
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å —Ü–≤–µ—Ç–æ–≤—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
    high_relevance = [w for w in data if w.get('relevance_score', 0) >= 8]
    medium_relevance = [w for w in data if 5 <= w.get('relevance_score', 0) < 8]
    low_relevance = [w for w in data if w.get('relevance_score', 0) < 5]
    
    output.append(f"  {colorize('‚òÖ', Colors.YELLOW)} {colorize('High Relevance (Score ‚â• 8):', Colors.BOLD + Colors.GREEN)} {colorize(f'{len(high_relevance):3d} papers', Colors.WHITE)}")
    output.append(f"  {colorize('‚òÜ', Colors.YELLOW)} {colorize('Medium Relevance (5-7):', Colors.BOLD + Colors.YELLOW)} {colorize(f'{len(medium_relevance):3d} papers', Colors.WHITE)}")
    output.append(f"  {colorize('‚óã', Colors.YELLOW)} {colorize('Low Relevance (Score < 5):', Colors.BOLD + Colors.RED)} {colorize(f'{len(low_relevance):3d} papers', Colors.WHITE)}")
    output.append("")
    
    # –ë—ã—Å—Ç—Ä—ã–π –æ–±–∑–æ—Ä –ø–æ –≥–æ–¥–∞–º —Å —Ü–≤–µ—Ç–æ–≤—ã–º–∏ –ø–æ–ª–æ—Å–∫–∞–º–∏
    if data:
        years = [w.get('publication_year', 0) for w in data if w.get('publication_year', 0) > 1900]
        if years:
            output.append(colorize("üìÖ PUBLICATION YEAR DISTRIBUTION:", Colors.BOLD + Colors.CYAN))
            year_counts = {}
            for year in years:
                year_counts[year] = year_counts.get(year, 0) + 1
            
            max_count = max(year_counts.values()) if year_counts else 1
            
            for year in sorted(year_counts.keys(), reverse=True)[:5]:
                count = year_counts[year]
                percentage = (count / len(data)) * 100
                bar_length = int((count / max_count) * 30)
                bar = colorize("‚ñà" * bar_length, Colors.GREEN)
                output.append(f"  {colorize(str(year), Colors.BOLD)}: {bar} {count:3d} papers ({percentage:5.1f}%)")
    output.append("")
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    # ========== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –°–¢–ê–¢–ï–ô –° –¶–í–ï–¢–û–í–û–ô –ö–û–î–ò–†–û–í–ö–û–ô ==========
    output.append(colorize("üìä DETAILED PAPER ANALYSIS", Colors.BOLD + Colors.PURPLE))
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    for i, work in enumerate(data, 1):
        # –¶–≤–µ—Ç–æ–≤–∞—è –∏–Ω–¥–∏–∫–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_score = work.get('relevance_score', 0)
        if relevance_score >= 8:
            relevance_color = Colors.GREEN
            relevance_icon = "‚òÖ"
        elif relevance_score >= 5:
            relevance_color = Colors.YELLOW
            relevance_icon = "‚òÜ"
        else:
            relevance_color = Colors.RED
            relevance_icon = "‚óã"
        
        # –¶–≤–µ—Ç–æ–≤–∞—è –∏–Ω–¥–∏–∫–∞—Ü–∏—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        citation_count = work.get('cited_by_count', 0)
        if citation_count == 0:
            citation_color = Colors.GREEN
            citation_icon = "üÜï"
        elif citation_count <= 3:
            citation_color = Colors.GREEN
            citation_icon = "üìà"
        elif citation_count <= 10:
            citation_color = Colors.YELLOW
            citation_icon = "üìä"
        else:
            citation_color = Colors.RED
            citation_icon = "üî•"
        
        # –ù–æ–º–µ—Ä –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å —Ü–≤–µ—Ç–æ–º
        output.append(colorize(f"PAPER #{i:03d}", Colors.BOLD + Colors.BLUE))
        output.append(colorize(f"‚îå{'‚îÄ' * 98}‚îê", Colors.CYAN))
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = work.get('title', 'No title available')
        output.append(f"‚îÇ {colorize('üìù TITLE:', Colors.BOLD)} {title[:90]}{'...' if len(title) > 90 else ''}")
        
        # –ê–≤—Ç–æ—Ä—ã
        authors = work.get('authors', [])
        if authors:
            output.append(f"‚îÇ {colorize('üë§ AUTHORS:', Colors.BOLD)} {', '.join(authors[:3])}")
            if len(authors) > 3:
                output.append(f"‚îÇ {' ' * 11}+ {len(authors) - 3} more authors")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ü–≤–µ—Ç–Ω—ã—Ö –±–ª–æ–∫–∞—Ö
        year = work.get('publication_year', 'N/A')
        journal = work.get('journal_name', 'N/A')
        
        metrics_line = f"‚îÇ {colorize('üìä METRICS:', Colors.BOLD)} "
        metrics_line += f"{citation_icon} {colorize(f'{citation_count} citations', citation_color)} | "
        metrics_line += f"{relevance_icon} {colorize(f'Score: {relevance_score}/10', relevance_color)} | "
        metrics_line += f"üìÖ {colorize(f'Year: {year}', Colors.CYAN)} | "
        metrics_line += f"üèõÔ∏è {colorize(journal[:30], Colors.PURPLE)}"
        output.append(metrics_line)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        oa_status = "‚úÖ Open Access" if work.get('is_oa') else "‚ùå Closed Access"
        output.append(f"‚îÇ {colorize('üîì ACCESS:', Colors.BOLD)} {colorize(oa_status, Colors.GREEN if work.get('is_oa') else Colors.RED)}")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if work.get('matched_keywords'):
            keywords = work.get('matched_keywords', [])
            output.append(f"‚îÇ {colorize('üè∑Ô∏è KEYWORDS:', Colors.BOLD)} {', '.join(keywords[:5])}")
            if len(keywords) > 5:
                output.append(f"‚îÇ {' ' * 12}+ {len(keywords) - 5} more")
        
        # DOI –∏ —Å—Å—ã–ª–∫–∞
        doi = work.get('doi', '')
        doi_url = work.get('doi_url', '')
        
        if doi:
            output.append(f"‚îÇ {colorize('üîó DOI:', Colors.BOLD)} {colorize(doi, Colors.BLUE)}")
            if doi_url:
                output.append(f"‚îÇ {' ' * 8}{colorize('üîó Link:', Colors.BOLD)} {colorize(doi_url, Colors.BLUE + Colors.UNDERLINE)}")
        
        output.append(colorize(f"‚îî{'‚îÄ' * 98}‚îò", Colors.CYAN))
        
        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
        if i < len(data):
            output.append(colorize("  " + "‚îÄ" * 98, Colors.CYAN))
            output.append("")
    
    output.append("")
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    # ========== –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –°–í–û–î–ö–ê –° –ì–†–ê–§–ò–ö–ê–ú–ò ASCII ==========
    if len(data) > 5:
        output.append(colorize("üìà STATISTICAL SUMMARY", Colors.BOLD + Colors.PURPLE))
        output.append(colorize("=" * 100, Colors.CYAN))
        output.append("")
        
        citations_list = [w.get('cited_by_count', 0) for w in data]
        relevance_list = [w.get('relevance_score', 0) for w in data]
        
        if citations_list:
            output.append(colorize("üìä CITATION ANALYSIS:", Colors.BOLD + Colors.CYAN))
            stats = [
                ("Average", np.mean(citations_list)),
                ("Median", np.median(citations_list)),
                ("Minimum", min(citations_list)),
                ("Maximum", max(citations_list)),
                ("Std Dev", np.std(citations_list))
            ]
            
            for name, value in stats:
                if isinstance(value, float):
                    value_str = f"{value:.2f}"
                else:
                    value_str = str(value)
                output.append(f"  {colorize(name + ':', Colors.BOLD):12} {colorize(value_str, Colors.GREEN)}")
            
            output.append("")
            
            # ASCII –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
            output.append(colorize("üìä CITATION DISTRIBUTION:", Colors.BOLD + Colors.CYAN))
            ranges = [(0, 0), (1, 2), (3, 5), (6, 10), (11, 20), (21, 50), (51, 100), (101, 1000)]
            max_count = 0
            
            # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            range_data = []
            for min_cit, max_cit in ranges:
                count = sum(1 for w in data if min_cit <= w.get('cited_by_count', 0) <= max_cit)
                if count > 0:
                    max_count = max(max_count, count)
                    if min_cit == max_cit:
                        range_str = f"Exactly {min_cit}"
                    else:
                        range_str = f"{min_cit}-{max_cit}"
                    range_data.append((range_str, count))
            
            # –í—ã–≤–æ–¥–∏–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
            for range_str, count in range_data:
                bar_length = int((count / max_count) * 40) if max_count > 0 else 0
                bar = colorize("‚ñà" * bar_length, Colors.GREEN)
                percentage = (count / len(data)) * 100
                output.append(f"  {range_str:12} citations: {bar} {count:3d} papers ({percentage:5.1f}%)")
            
            output.append("")
        
        if relevance_list:
            output.append(colorize("üéØ RELEVANCE SCORE ANALYSIS:", Colors.BOLD + Colors.PURPLE))
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å —Ü–≤–µ—Ç–Ω—ã–º–∏ –∑–≤–µ–∑–¥–∞–º–∏
            relevance_counts = {score: 0 for score in range(1, 11)}
            for score in relevance_list:
                rounded = min(int(score), 10)
                relevance_counts[rounded] = relevance_counts.get(rounded, 0) + 1
            
            for score in range(10, 0, -1):
                count = relevance_counts.get(score, 0)
                if count > 0:
                    percentage = (count / len(data)) * 100
                    stars = colorize("‚òÖ" * min(score, 5), Colors.YELLOW) + colorize("‚òÜ" * max(5 - score, 0), Colors.WHITE)
                    bar_length = int((count / max(relevance_counts.values())) * 30) if max(relevance_counts.values()) > 0 else 0
                    bar = colorize("‚ñà" * bar_length, Colors.YELLOW)
                    output.append(f"  Score {score:2d}/10 {stars}: {bar} {count:3d} papers ({percentage:5.1f}%)")
            
            output.append("")
    
    # ========== –¢–û–ü –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô –° –¶–í–ï–¢–ù–´–ú–ò –ë–ï–ô–î–ñ–ê–ú–ò ==========
    if len(data) > 10:
        output.append(colorize("üèÜ TOP RECOMMENDATIONS", Colors.BOLD + Colors.GREEN))
        output.append(colorize("=" * 100, Colors.CYAN))
        output.append("")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        output.append(colorize("üéñÔ∏è Highest Relevance & Most Recent:", Colors.BOLD + Colors.YELLOW))
        sorted_by_relevance = sorted(data, key=lambda x: (-x.get('relevance_score', 0), 
                                                          -x.get('publication_year', 0)))
        
        for i, work in enumerate(sorted_by_relevance[:5], 1):
            title = work.get('title', '')[:70] + "..." if len(work.get('title', '')) > 70 else work.get('title', '')
            badge = colorize(f"#{i}", Colors.BOLD + Colors.WHITE + Colors.BG_BLUE)
            output.append(f"  {badge} {title}")
            output.append(f"     {colorize('Year:', Colors.CYAN)} {work.get('publication_year', 'N/A')} | "
                         f"{colorize('Citations:', Colors.GREEN)} {work.get('cited_by_count', 0)} | "
                         f"{colorize('Score:', Colors.YELLOW)} {work.get('relevance_score', 0)}/10")
        
        output.append("")
        
        # –°–∞–º—ã–µ —Ü–∏—Ç–∏—Ä—É–µ–º—ã–µ —Å—Ä–µ–¥–∏ –º–∞–ª–æ—Ü–∏—Ç–∏—Ä—É–µ–º—ã—Ö
        output.append(colorize("üî• Most Cited (among under-cited):", Colors.BOLD + Colors.RED))
        cited_papers = [w for w in data if w.get('cited_by_count', 0) > 0]
        if cited_papers:
            most_cited = sorted(cited_papers, key=lambda x: -x.get('cited_by_count', 0))
            for i, work in enumerate(most_cited[:3], 1):
                title = work.get('title', '')[:70] + "..." if len(work.get('title', '')) > 70 else work.get('title', '')
                badge = colorize(f"#{i}", Colors.BOLD + Colors.WHITE + Colors.BG_RED)
                output.append(f"  {badge} {title}")
                output.append(f"     {colorize('Citations:', Colors.RED)} {work.get('cited_by_count', 0)} | "
                             f"{colorize('Year:', Colors.CYAN)} {work.get('publication_year', 'N/A')}")
        
        output.append("")
        
        # –°–∞–º—ã–µ –Ω–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        output.append(colorize("üÜï Newest Publications:", Colors.BOLD + Colors.BLUE))
        recent_papers = sorted(data, key=lambda x: -x.get('publication_year', 0))
        for i, work in enumerate(recent_papers[:3], 1):
            title = work.get('title', '')[:70] + "..." if len(work.get('title', '')) > 70 else work.get('title', '')
            badge = colorize(f"#{i}", Colors.BOLD + Colors.WHITE + Colors.BG_GREEN)
            output.append(f"  {badge} {title}")
            output.append(f"     {colorize('Year:', Colors.GREEN)} {work.get('publication_year', 'N/A')} | "
                         f"{colorize('Citations:', Colors.CYAN)} {work.get('cited_by_count', 0)}")
    
    # ========== –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –° –¶–í–ï–¢–ù–´–ú–ò –†–ê–ó–î–ï–õ–ê–ú–ò ==========
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append(colorize("üí° CONCLUSION & RECOMMENDATIONS", Colors.BOLD + Colors.PURPLE))
    output.append(colorize("=" * 100, Colors.CYAN))
    output.append("")
    
    conclusions = [
        colorize("üîç KEY INSIGHTS:", Colors.BOLD + Colors.CYAN),
        f"‚Ä¢ {colorize('Emerging Research:', Colors.GREEN)} These papers may represent new trends",
        f"‚Ä¢ {colorize('Hidden Gems:', Colors.YELLOW)} Low citations don't indicate low quality",
        f"‚Ä¢ {colorize('Review Starting Points:', Colors.BLUE)} Ideal for literature reviews",
        f"‚Ä¢ {colorize('Cross-disciplinary:', Colors.PURPLE)} May contain novel methodologies",
        "",
        colorize("üöÄ RECOMMENDED ACTIONS:", Colors.BOLD + Colors.GREEN),
        f"1. {colorize('Review high-relevance papers', Colors.WHITE)} for potential citations",
        f"2. {colorize('Use as starting points', Colors.WHITE)} for systematic reviews",
        f"3. {colorize('Identify research gaps', Colors.WHITE)} and opportunities",
        f"4. {colorize('Track emerging authors', Colors.WHITE)} in this field",
        "",
        colorize("üìã REPORT METADATA:", Colors.BOLD + Colors.YELLOW),
        f"‚Ä¢ {colorize('Generated by:', Colors.WHITE)} CTA Article Recommender Pro",
        f"‚Ä¢ {colorize('Report ID:', Colors.WHITE)} {hashlib.md5(str(datetime.now()).encode()).hexdigest()[:12].upper()}",
        f"‚Ä¢ {colorize('Data source:', Colors.WHITE)} OpenAlex API",
        f"‚Ä¢ {colorize('Analysis date:', Colors.WHITE)} {current_date}",
        f"‚Ä¢ {colorize('Input DOIs:', Colors.WHITE)} {metadata.get('original_dois_count', 'Unknown') if metadata else 'Unknown'}",
        f"‚Ä¢ {colorize('Analysis filters:', Colors.WHITE)} Preserved in metadata section",
        "",
        colorize("¬© CTA - Chimica Techno Acta | üåê https://chimicatechnoacta.ru", Colors.CYAN),
        colorize("üìù This report is for research purposes only.", Colors.YELLOW),
        colorize("‚úÖ Always verify information with original sources.", Colors.GREEN),
        "",
        colorize("üìå End of Report", Colors.BOLD + Colors.PURPLE),
        colorize("=" * 100, Colors.CYAN)
    ]
    
    output.extend(conclusions)
    
    return "\n".join(output)

# ============================================================================
# –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ò–ù–¢–ï–†–§–ï–ô–°–ê
# ============================================================================

def create_progress_bar(current_step: int, total_steps: int):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä –º–∞—Å—Ç–µ—Ä-–ø—Ä–æ—Ü–µ—Å—Å–∞"""
    progress = current_step / total_steps
    
    st.markdown(f"""
    <div class="progress-container">
        <div class="progress-bar" style="width: {progress * 100}%"></div>
    </div>
    <div class="step-indicator">
        <span class="{'active' if current_step >= 1 else ''}">üì• Data Input</span>
        <span class="{'active' if current_step >= 2 else ''}">üîç Analysis</span>
        <span class="{'active' if current_step >= 3 else ''}">üéØ Topic Selection</span>
        <span class="{'active' if current_step >= 4 else ''}">üìä Results</span>
    </div>
    """, unsafe_allow_html=True)

def create_back_button():
    """–°–æ–∑–¥–∞–µ—Ç –∫–Ω–æ–ø–∫—É –≤–æ–∑–≤—Ä–∞—Ç–∞ –Ω–∞–∑–∞–¥"""
    if st.session_state.current_step > 1:
        if st.button("‚Üê Back", key="back_button", use_container_width=False):
            # –ü—Ä–∏ –≤–æ–∑–≤—Ä–∞—Ç–µ –Ω–∞ —à–∞–≥ 3, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –∑–∞–Ω–æ–≤–æ
            if st.session_state.current_step == 4:
                if 'relevant_works' in st.session_state:
                    del st.session_state['relevant_works']
                if 'top_keywords' in st.session_state:
                    del st.session_state['top_keywords']
            
            st.session_state.current_step -= 1
            st.rerun()

def create_metric_card_compact(title: str, value, icon: str = "üìä"):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –∫–∞—Ä—Ç–æ—á–∫—É —Å –º–µ—Ç—Ä–∏–∫–æ–π"""
    st.markdown(f"""
    <div class="metric-card">
        <h4>{icon} {title}</h4>
        <div class="value">{value}</div>
    </div>
    """, unsafe_allow_html=True)

def create_result_card_compact(work: dict, index: int):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –∫–∞—Ä—Ç–æ—á–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    citation_count = work.get('cited_by_count', 0)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –±–∞–¥–∂–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    if citation_count == 0:
        badge_color = "#4CAF50"
        badge_text = "0 citations"
    elif citation_count <= 3:
        badge_color = "#4CAF50"
        badge_text = f"{citation_count} citation{'s' if citation_count > 1 else ''}"
    elif citation_count <= 10:
        badge_color = "#FF9800"
        badge_text = f"{citation_count} citations"
    else:
        badge_color = "#f44336"
        badge_text = f"{citation_count} citations"
    
    oa_badge = 'üîì' if work.get('is_oa') else 'üîí'
    doi_url = work.get('doi_url', '')
    title = work.get('title', 'No title')
    authors = ', '.join(work.get('authors', [])[:2])
    if len(work.get('authors', [])) > 2:
        authors += ' et al.'
    
    st.markdown(f"""
    <div class="result-card">
        <div style="display: flex; justify-content: space-between; margin-bottom: 8px;">
            <div>
                <span style="font-weight: 600; color: #667eea; margin-right: 8px;">#{index}</span>
                <span style="background: {badge_color}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.75rem;">
                    {badge_text}
                </span>
                <span style="background: #e3f2fd; padding: 2px 8px; border-radius: 12px; font-size: 0.75rem; margin-left: 5px;">
                    Score: {work.get('relevance_score', 0)}
                </span>
            </div>
            <span style="color: #666; font-size: 0.8rem;">{work.get('publication_year', '')}</span>
        </div>
        <div style="font-weight: 600; font-size: 0.95rem; margin-bottom: 5px; line-height: 1.3;">{title}</div>
        <div style="color: #555; font-size: 0.85rem; margin-bottom: 5px;">üë§ {authors}</div>
        <div style="display: flex; justify-content: space-between; align-items: center; margin-top: 8px;">
            <span>{oa_badge} {work.get('journal_name', '')[:30]}</span>
            <a href="{doi_url}" target="_blank" style="color: #2196F3; text-decoration: none; font-size: 0.85rem;">
                üîó View Article
            </a>
        </div>
    </div>
    """, unsafe_allow_html=True)

def create_topic_selection_ui():
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤—ã–±–æ—Ä–∞ —Ç–µ–º—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
    st.markdown("<h4>üéØ Select Research Topic</h4>", unsafe_allow_html=True)
    
    topics = st.session_state.topic_counter.most_common()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 8 —Ç–µ–º –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º –≤–∏–¥–µ
    cols = st.columns(2)
    for idx, (topic, count) in enumerate(topics[:8]):
        with cols[idx % 2]:
            is_selected = st.session_state.get('selected_topic') == topic
            st.markdown(f"""
            <div class="topic-card {'selected' if is_selected else ''}" 
                 onclick="this.style.background='#667eea10';">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <div style="font-weight: 600; font-size: 0.9rem;">{topic[:70]}{'...' if len(topic) > 70 else ''}</div>
                    <span style="background: #667eea; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.75rem;">
                        {count} papers
                    </span>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            if st.button(f"Select", key=f"select_{idx}", 
                        use_container_width=True,
                        type="primary" if is_selected else "secondary"):
                st.session_state.selected_topic = topic
                
                # –ù–∞—Ö–æ–¥–∏–º ID —Ç–µ–º—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                for work in st.session_state.works_data:
                    if work.get('primary_topic') == topic:
                        topic_id = work.get('topic_id')
                        if topic_id:
                            st.session_state.selected_topic_id = topic_id
                            break
                
                st.rerun()
    
    # –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ—è–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ —Ç–µ–º—ã)
    if 'selected_topic' in st.session_state:
        st.markdown("---")
        st.markdown("<h4>‚öôÔ∏è Analysis Filters</h4>", unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # –§–∏–ª—å—Ç—Ä –ø–æ –≥–æ–¥–∞–º - –¢–û–õ–¨–ö–û –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –≥–æ–¥–∞
            current_year = datetime.now().year
            years = list(range(current_year - 2, current_year + 1))  # –¢–æ–ª—å–∫–æ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≥–æ–¥–∞
            
            selected_years = st.multiselect(
                "Publication Years:",
                options=years,
                default=[current_year - 2, current_year - 1, current_year],
                help="Select publication years (last 3 years only)"
            )
            
            if not selected_years:
                st.warning("Please select at least one year")
                selected_years = [current_year - 2, current_year - 1, current_year]
            
            st.session_state.selected_years = selected_years
        
        with col2:
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ–ø—Ü–∏–∏)
            citation_options = [
                ("0 citations", "0"),
                ("1 citation", "1"),
                ("2 citations", "2"),
                ("3 citations", "3"),
                ("4 citations", "4"),
                ("5 citations", "5"),
                ("6 citations", "6"),
                ("7 citations", "7"),
                ("8 citations", "8"),
                ("9 citations", "9"),
                ("10 citations", "10"),
                ("0-2 citations", "0-2"),
                ("0-3 citations", "0-3"),
                ("0-5 citations", "0-5"),
                ("1-3 citations", "1-3"),
                ("1-5 citations", "1-5"),
                ("2-5 citations", "2-5"),
                ("3-5 citations", "3-5"),
                ("5-10 citations", "5-10"),
                ("0-1,3-4 (multiple ranges)", "0-1,3-4"),
                ("Custom...", "custom")
            ]
            
            selected_option = st.selectbox(
                "Citation Ranges:",
                options=[opt[0] for opt in citation_options],
                index=2,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é "0-2 citations"
                help="Select citation ranges (0-10 only)"
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            if selected_option == "Custom...":
                custom_input = st.text_input(
                    "Enter custom ranges (e.g., '0-2,4,5-7'):",
                    value="0-2",
                    help="Enter comma-separated values or ranges (0-10 only)"
                )
                if custom_input:
                    citation_ranges = parse_citation_ranges(custom_input)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 0-10
                    valid_ranges = []
                    for start, end in citation_ranges:
                        if 0 <= start <= 10 and 0 <= end <= 10:
                            valid_ranges.append((start, end))
                        else:
                            st.warning(f"Range {start}-{end} is outside 0-10. It will be ignored.")
                    
                    if valid_ranges:
                        st.session_state.selected_ranges = valid_ranges
                        st.info(f"Selected ranges: {format_citation_ranges(valid_ranges)}")
                    else:
                        st.session_state.selected_ranges = [(0, 2)]
                        st.warning("Using default range: 0-2")
                else:
                    st.session_state.selected_ranges = [(0, 2)]
            else:
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –æ–ø—Ü–∏–∏
                range_str = next(opt[1] for opt in citation_options if opt[0] == selected_option)
                citation_ranges = parse_citation_ranges(range_str)
                st.session_state.selected_ranges = citation_ranges
                st.info(f"Selected: {selected_option}")
        
        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üîç Start Deep Analysis", type="primary", use_container_width=True, key="start_analysis"):
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if 'relevant_works' in st.session_state:
                    del st.session_state['relevant_works']
                if 'top_keywords' in st.session_state:
                    del st.session_state['top_keywords']
                
                st.session_state.current_step = 4
                st.rerun()

# ============================================================================
# –®–ê–ì–ò –ú–ê–°–¢–ï–†-–ü–†–û–¶–ï–°–°–ê
# ============================================================================

def step_data_input():
    """–®–∞–≥ 1: –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)"""
    create_back_button()
    
    st.markdown("""
    <div class="step-card">
        <h3 style="margin: 0; font-size: 1.3rem;">üì• Step 1: Input Research DOIs</h3>
        <p style="margin: 5px 0; font-size: 0.9rem;">Enter DOI identifiers to analyze topics and keywords.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –≤–≤–æ–¥
    doi_input = st.text_area(
        "**DOI Input** (one per line or comma-separated):",
        height=150,
        placeholder="Example:\n10.1016/j.jpowsour.2020.228660\n10.1038/s41560-020-00734-0\n10.1021/acsenergylett.1c00123",
        help="Enter up to 300 DOI identifiers"
    )
    
    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("üöÄ Start Analysis", type="primary", use_container_width=True):
            if doi_input:
                dois = parse_doi_input(doi_input)
                if dois:
                    st.session_state.dois = dois
                    st.session_state.current_step = 2
                    st.rerun()
                else:
                    st.error("‚ùå No valid DOI identifiers found.")
            else:
                st.error("‚ùå Please enter at least one DOI")
    
    with col2:
        if st.button("üîÑ Clear", use_container_width=True):
            st.rerun()

def step_analysis():
    """–®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)"""
    create_back_button()
    
    st.markdown("""
    <div class="step-card">
        <h3 style="margin: 0; font-size: 1.3rem;">üîç Step 2: Analysis in Progress</h3>
        <p style="margin: 5px 0; font-size: 0.9rem;">Fetching data from OpenAlex...</p>
    </div>
    """, unsafe_allow_html=True)
    
    if 'dois' not in st.session_state:
        st.error("‚ùå No data to analyze. Please go back to Step 1.")
        return
    
    dois = st.session_state.dois
    
    # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    col1, col2, col3 = st.columns(3)
    with col1:
        create_metric_card_compact("DOIs", len(dois), "üî¢")
    with col2:
        create_metric_card_compact("Est. Time", f"{len(dois)//10}s", "‚è±Ô∏è")
    with col3:
        create_metric_card_compact("API Rate", "8/sec", "‚ö°")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    with st.spinner("Fetching data..."):
        results, successful, failed = fetch_works_by_dois_sync(dois)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    works_data = []
    topic_counter = Counter()
    titles = []
    
    for result in results:
        if result.get('success') and result.get('data'):
            work = result['data']
            enriched = enrich_work_data(work)
            
            if enriched.get('primary_topic'):
                topic_counter[enriched['primary_topic']] += 1
            
            works_data.append(enriched)
            titles.append(enriched.get('title', ''))
    
    # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    keyword_counter = analyze_keywords_parallel(titles)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.session_state.works_data = works_data
    st.session_state.topic_counter = topic_counter
    st.session_state.keyword_counter = keyword_counter
    st.session_state.successful = successful
    st.session_state.failed = failed
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    st.markdown(f"""
    <div class="info-message">
        <div style="display: flex; justify-content: space-between; align-items: center;">
            <div>
                <strong>‚úÖ Analysis Complete!</strong><br>
                Successfully processed {successful} papers
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    col1, col2, col3 = st.columns(3)
    with col1:
        create_metric_card_compact("Successful", successful, "‚úÖ")
    with col2:
        create_metric_card_compact("Failed", failed, "‚ùå")
    with col3:
        create_metric_card_compact("Topics", len(topic_counter), "üè∑Ô∏è")
    
    # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
    st.markdown("---")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("üéØ Continue to Topic Selection", type="primary", use_container_width=True):
            st.session_state.current_step = 3
            st.rerun()

def step_topic_selection():
    """–®–∞–≥ 3: –í—ã–±–æ—Ä —Ç–µ–º—ã (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)"""
    create_back_button()
    
    st.markdown("""
    <div class="step-card">
        <h3 style="margin: 0; font-size: 1.3rem;">üéØ Step 3: Select Research Topic</h3>
        <p style="margin: 5px 0; font-size: 0.9rem;">Choose a topic for deep analysis of fresh papers.</p>
    </div>
    """, unsafe_allow_html=True)
    
    if not st.session_state.works_data:
        st.error("‚ùå No data available. Please start from Step 1.")
        return
    
    create_topic_selection_ui()

def step_results():
    """–®–∞–≥ 4: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)"""
    create_back_button()
    
    st.markdown("""
    <div class="step-card">
        <h3 style="margin: 0; font-size: 1.3rem;">üìä Step 4: Analysis Results</h3>
        <p style="margin: 5px 0; font-size: 0.9rem;">Fresh papers in your research area.</p>
    </div>
    """, unsafe_allow_html=True)
    
    if 'selected_topic_id' not in st.session_state:
        st.error("‚ùå Topic not selected. Please go back to Step 3.")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
    selected_years = st.session_state.get('selected_years', [])
    if not selected_years:
        current_year = datetime.now().year
        selected_years = [current_year - 2, current_year - 1, current_year]
        st.session_state.selected_years = selected_years
    
    selected_ranges = st.session_state.get('selected_ranges', [])
    if not selected_ranges:
        selected_ranges = [(0, 2)]
        st.session_state.selected_ranges = selected_ranges
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ
    if 'relevant_works' not in st.session_state:
        with st.spinner("Searching for fresh papers..."):
            top_keywords = [kw for kw, _ in st.session_state.keyword_counter.most_common(10)]
            st.session_state.top_keywords = top_keywords
            
            relevant_works = analyze_works_for_topic(
                st.session_state.selected_topic_id,
                top_keywords,
                max_citations=10,
                max_works=2000,
                top_n=100,
                year_filter=selected_years,
                citation_ranges=selected_ranges
            )
        
        st.session_state.relevant_works = relevant_works
    else:
        relevant_works = st.session_state.relevant_works
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    metadata = {
        'generated_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'topic_name': st.session_state.get('selected_topic', 'Unknown'),
        'total_papers': len(relevant_works),
        'original_dois': st.session_state.get('dois', []),
        'original_dois_count': len(st.session_state.get('dois', [])),
        'analysis_filters': {
            'years': selected_years,
            'citation_ranges': selected_ranges,
            'citation_ranges_display': format_citation_ranges(selected_ranges),
            'max_citations': 10,
            'max_works': 2000,
            'top_n': 100
        },
        'keywords_used': st.session_state.get('top_keywords', [])
    }
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        create_metric_card_compact("Papers Found", len(relevant_works), "üìÑ")
    with col2:
        if relevant_works:
            avg_citations = np.mean([w.get('cited_by_count', 0) for w in relevant_works])
            create_metric_card_compact("Avg Citations", f"{avg_citations:.1f}", "üìà")
        else:
            create_metric_card_compact("Avg Citations", "0", "üìà")
    with col3:
        oa_count = sum(1 for w in relevant_works if w.get('is_oa'))
        create_metric_card_compact("Open Access", oa_count, "üîì")
    with col4:
        current_year = datetime.now().year
        recent_count = sum(1 for w in relevant_works if w.get('publication_year', 0) >= current_year - 2)
        create_metric_card_compact("Recent (‚â§2y)", recent_count, "üïí")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
    st.markdown(f"""
    <div style="margin: 10px 0; font-size: 0.85rem; color: #666;">
        <strong>Active filters:</strong> Years: {', '.join(map(str, selected_years))} | 
        Citation ranges: {format_citation_ranges(selected_ranges)} |
        Input DOIs: {len(st.session_state.get('dois', []))}
    </div>
    """, unsafe_allow_html=True)
    
    if not relevant_works:
        st.warning("""
        <div class="warning-message">
            <strong>‚ö†Ô∏è No papers match your filters</strong><br>
            This might happen when:<br>
            1. Current year selected with high citation threshold (papers might not have enough citations yet)<br>
            2. Very specific citation range selected<br>
            3. Topic has limited publications in selected years<br>
            <br>
            Try adjusting your filters in Step 3.
        </div>
        """, unsafe_allow_html=True)
    else:
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ –∫–∞—Ä—Ç–æ—á–µ–∫
        st.markdown("<h4>üéØ Recommended Papers:</h4>", unsafe_allow_html=True)
        
        for idx, work in enumerate(relevant_works[:10], 1):
            create_result_card_compact(work, idx)
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        st.markdown("<h4>üìã Detailed View:</h4>", unsafe_allow_html=True)
        
        display_data = []
        for i, work in enumerate(relevant_works, 1):
            doi_url = work.get('doi_url', '')
            title = work.get('title', '')
            
            display_data.append({
                '#': i,
                'Title': title[:60] + '...' if len(title) > 60 else title,
                'Citations': work.get('cited_by_count', 0),
                'Relevance': work.get('relevance_score', 0),
                'Year': work.get('publication_year', ''),
                'Journal': work.get('journal_name', '')[:20],
                'DOI': doi_url if doi_url else 'N/A',
                'OA': '‚úÖ' if work.get('is_oa') else '‚ùå',
                'Authors': ', '.join(work.get('authors', [])[:2])
            })
        
        df = pd.DataFrame(display_data)
        
        st.dataframe(
            df,
            use_container_width=True,
            height=300,
            column_config={
                "DOI": st.column_config.TextColumn(
                    "DOI",
                    help="Click to copy or open in browser",
                    width="medium"
                ),
                "Relevance": st.column_config.ProgressColumn(
                    "Relevance",
                    help="Relevance score (higher is better)",
                    format="%d",
                    min_value=1,
                    max_value=10
                )
            }
        )
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        st.markdown("<h4>üì• Export Results:</h4>", unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            csv = generate_csv(relevant_works, metadata)
            st.download_button(
                label="üìä CSV",
                data=csv,
                file_name=f"under_cited_papers_{st.session_state.get('selected_topic', 'results').replace(' ', '_')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            excel_data = generate_excel(relevant_works, metadata)
            st.download_button(
                label="üìà Excel",
                data=excel_data,
                file_name=f"under_cited_papers_{st.session_state.get('selected_topic', 'results').replace(' ', '_')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        
        with col3:
            txt_data = generate_txt(relevant_works, 
                                   st.session_state.get('selected_topic', 'Results'), 
                                   metadata)
            st.download_button(
                label="üìù TXT",
                data=txt_data,
                file_name=f"under_cited_papers_{st.session_state.get('selected_topic', 'results').replace(' ', '_')}.txt",
                mime="text/plain",
                use_container_width=True
            )
        
        with col4:
            pdf_data = generate_pdf(relevant_works[:50], 
                                   st.session_state.get('selected_topic', 'Results'), 
                                   metadata)
            st.download_button(
                label="üìÑ PDF",
                data=pdf_data,
                file_name=f"under_cited_papers_{st.session_state.get('selected_topic', 'results').replace(' ', '_')}.pdf",
                mime="application/pdf",
                use_container_width=True
            )
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        st.info(f"""
        **üìã Report Information:**
        - Generated on: {metadata['generated_date']}
        - Input DOIs: {metadata['original_dois_count']}
        - Analysis filters preserved in exported files
        - Use identical inputs to reproduce results
        """)
        
        # –ö–Ω–æ–ø–∫–∞ –Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üîÑ Start New Analysis", use_container_width=True):
                for key in ['relevant_works', 'selected_topic', 'selected_topic_id', 
                          'selected_years', 'selected_ranges', 'top_keywords',
                          'works_data', 'topic_counter', 'keyword_counter',
                          'successful', 'failed', 'dois']:
                    if key in st.session_state:
                        del st.session_state[key]
                st.session_state.current_step = 1
                st.rerun()

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 1
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π)
    st.markdown("""
    <h1 class="main-header">üî¨ CTA Article Recommender Pro</h1>
    <p style="font-size: 1rem; color: #666; margin-bottom: 1.5rem;">
    Discover fresh papers using AI-powered analysis
    </p>
    """, unsafe_allow_html=True)
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
    create_progress_bar(st.session_state.current_step, 4)
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞
    clear_old_cache()
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞
    if st.session_state.current_step == 1:
        step_data_input()
    elif st.session_state.current_step == 2:
        step_analysis()
    elif st.session_state.current_step == 3:
        step_topic_selection()
    elif st.session_state.current_step == 4:
        step_results()
    
    # –§—É—Ç–µ—Ä
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #888; font-size: 0.8rem; margin-top: 1rem;">
        <p>¬© CTA, https://chimicatechnoacta.ru / developed by daM¬©</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()





