import streamlit as st
import requests
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
from datetime import datetime, timedelta
import json
import asyncio
import aiohttp
import time
import sqlite3
import os
from pathlib import Path
import hashlib
import joblib
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from ratelimit import limits, sleep_and_retry
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.set_page_config(
    page_title="CTA Article Recommendation Pro+",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAlex API
OPENALEX_BASE_URL = "https://api.openalex.org"
MAILTO = "your-email@example.com"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à email –¥–ª—è polite pool
POLITE_POOL_HEADER = {'User-Agent': f'CTA-App (mailto:{MAILTO})'}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ rate limit
RATE_LIMIT_PER_SECOND = 8  # 8 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è polite pool
BATCH_SIZE = 50  # –†–∞–∑–º–µ—Ä batch –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç
CURSOR_PAGE_SIZE = 200  # –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è cursor pagination
MAX_WORKERS_ASYNC = 3  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
MAX_RETRIES = 3  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
INITIAL_DELAY = 1  # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ retry
MAX_DELAY = 60  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
CACHE_DIR = Path("./cache")
CACHE_DB = CACHE_DIR / "openalex_cache.db"
CACHE_EXPIRY_DAYS = 30  # –î–Ω–µ–π —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
CACHE_DIR.mkdir(exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
nltk.download('stopwords', quiet=True)
COMMON_WORDS = {
    'study', 'studies', 'research', 'paper', 'article', 'review', 'analysis', 'analyses',
    'investigation', 'investigations', 'effect', 'effects', 'property', 'properties',
    'performance', 'behavior', 'behaviour', 'characterization', 'characterisation',
    'synthesis', 'development', 'preparation', 'fabrication', 'application', 'applications',
    'method', 'methods', 'approach', 'approaches', 'result', 'results', 'discussion',
    'conclusion', 'conclusions', 'introduction', 'experimental', 'experiment', 'experiments',
    'measurement', 'measurements', 'observation', 'observations', 'technique', 'techniques',
    'technology', 'technologies', 'material', 'materials', 'system', 'systems',
    'process', 'processes', 'structure', 'structures', 'model', 'models',
    'based', 'using', 'used', 'use', 'high', 'low', 'temperature', 'temperatures',
    'pressure', 'different', 'various', 'several', 'important', 'significant',
    'novel', 'new', 'recent', 'current', 'potential', 'possible', 'first',
    'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth',
    'tenth', 'good', 'better', 'best', 'poor', 'higher', 'lower', 'strong',
    'weak', 'large', 'small', 'great', 'major', 'minor', 'main', 'primary',
    'secondary', 'critical', 'essential', 'general', 'specific', 'special',
    'particular', 'similar', 'different', 'various', 'several', 'multiple',
    'numerous', 'common', 'unusual', 'typical', 'atypical', 'standard',
    'advanced', 'basic', 'fundamental', 'theoretical', 'practical', 'experimental',
    'computational', 'numerical', 'analytical', 'theoretical', 'practical'
}

ALL_STOPWORDS = set(stopwords.words('english')).union(COMMON_WORDS)

# ============================================================================
# –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ù–ê –£–†–û–í–ù–ï SQLite
# ============================================================================

def init_cache_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    conn = sqlite3.connect(CACHE_DB)
    cursor = conn.cursor()
    
    # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç –ø–æ DOI
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS works_cache (
            doi TEXT PRIMARY KEY,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME
        )
    ''')
    
    # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS topic_works_cache (
            topic_id TEXT,
            cursor_key TEXT,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME,
            PRIMARY KEY (topic_id, cursor_key)
        )
    ''')
    
    # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–µ–º
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS topics_cache (
            topic_id TEXT PRIMARY KEY,
            data TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            expires_at DATETIME
        )
    ''')
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_works_expires ON works_cache(expires_at)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic_works_expires ON topic_works_cache(expires_at)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_topics_expires ON topics_cache(expires_at)')
    
    conn.commit()
    conn.close()

def get_cache_key(prefix: str, key: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –∫—ç—à–∞"""
    return hashlib.md5(f"{prefix}:{key}".encode()).hexdigest()

@st.cache_resource
def get_db_connection():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞"""
    init_cache_db()
    return sqlite3.connect(CACHE_DB, check_same_thread=False)

def cache_work(doi: str, data: dict):
    """–ö—ç—à–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç—ã"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=CACHE_EXPIRY_DAYS)
    
    cursor.execute('''
        INSERT OR REPLACE INTO works_cache (doi, data, expires_at)
        VALUES (?, ?, ?)
    ''', (doi, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_work(doi: str) -> Optional[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç—ã"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM works_cache 
        WHERE doi = ? AND (expires_at IS NULL OR expires_at > ?)
    ''', (doi, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def cache_topic_works(topic_id: str, cursor_key: str, data: dict):
    """–ö—ç—à–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=7)  # –ö—ç—à —Ç–µ–º –Ω–∞ 7 –¥–Ω–µ–π
    
    cursor.execute('''
        INSERT OR REPLACE INTO topic_works_cache (topic_id, cursor_key, data, expires_at)
        VALUES (?, ?, ?, ?)
    ''', (topic_id, cursor_key, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_topic_works(topic_id: str, cursor_key: str) -> Optional[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM topic_works_cache 
        WHERE topic_id = ? AND cursor_key = ? 
        AND (expires_at IS NULL OR expires_at > ?)
    ''', (topic_id, cursor_key, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def cache_topic_stats(topic_id: str, data: dict):
    """–ö—ç—à–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–º—ã"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    expires_at = datetime.now() + timedelta(days=30)  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ 30 –¥–Ω–µ–π
    
    cursor.execute('''
        INSERT OR REPLACE INTO topics_cache (topic_id, data, expires_at)
        VALUES (?, ?, ?)
    ''', (topic_id, json.dumps(data), expires_at))
    
    conn.commit()

def get_cached_topic_stats(topic_id: str) -> Optional[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–º—ã"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT data FROM topics_cache 
        WHERE topic_id = ? AND (expires_at IS NULL OR expires_at > ?)
    ''', (topic_id, datetime.now()))
    
    result = cursor.fetchone()
    if result:
        return json.loads(result[0])
    return None

def clear_old_cache():
    """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('DELETE FROM works_cache WHERE expires_at <= ?', (datetime.now(),))
    cursor.execute('DELETE FROM topic_works_cache WHERE expires_at <= ?', (datetime.now(),))
    cursor.execute('DELETE FROM topics_cache WHERE expires_at <= ?', (datetime.now(),))
    
    conn.commit()

# ============================================================================
# ASYNCIO + AIOHTTP –î–õ–Ø –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–• –ó–ê–ü–†–û–°–û–í
# ============================================================================

class OpenAlexAsyncClient:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è OpenAlex API —Å rate limiting"""
    
    def __init__(self):
        self.session = None
        self.semaphore = asyncio.Semaphore(MAX_WORKERS_ASYNC)
        self.request_count = 0
        self.start_time = time.time()
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers=POLITE_POOL_HEADER,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    @retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=INITIAL_DELAY, max=MAX_DELAY),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def make_request(self, url: str) -> Optional[dict]:
        """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å rate limiting –∏ retry –ª–æ–≥–∏–∫–æ–π"""
        async with self.semaphore:
            # Rate limiting: 8 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
            elapsed = time.time() - self.start_time
            expected_time = self.request_count / RATE_LIMIT_PER_SECOND
            
            if elapsed < expected_time:
                wait_time = expected_time - elapsed
                await asyncio.sleep(wait_time)
            
            try:
                async with self.session.get(url) as response:
                    self.request_count += 1
                    
                    if response.status == 429:
                        retry_after = int(response.headers.get('Retry-After', 5))
                        logger.warning(f"Rate limited. Waiting {retry_after} seconds")
                        await asyncio.sleep(retry_after)
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=429
                        )
                    
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 404:
                        logger.warning(f"Resource not found: {url}")
                        return None
                    else:
                        logger.error(f"HTTP {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.warning(f"Timeout: {url}")
                raise
            except Exception as e:
                logger.error(f"Error: {url} - {str(e)}")
                raise
    
    async def fetch_works_by_dois_batch(self, dois: List[str]) -> List[Optional[dict]]:
        """Batch –∑–∞–ø—Ä–æ—Å —Ä–∞–±–æ—Ç –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º DOI –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
        if not dois:
            return []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_results = []
        uncached_dois = []
        
        for doi in dois:
            cached = get_cached_work(doi)
            if cached:
                cached_results.append(cached)
            else:
                uncached_dois.append(doi)
        
        if not uncached_dois:
            return cached_results
        
        # –î–µ–ª–∞–µ–º batch –∑–∞–ø—Ä–æ—Å –¥–ª—è –Ω–µ–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö DOI
        logger.info(f"Fetching {len(uncached_dois)} works via batch API")
        
        # OpenAlex –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç filter –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º DOI —á–µ—Ä–µ–∑ |
        doi_filter = "|".join(uncached_dois)
        url = f"{OPENALEX_BASE_URL}/works?filter=doi:{doi_filter}&per-page=200"
        
        try:
            data = await self.make_request(url)
            if data and 'results' in data:
                results = data['results']
                
                # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for work in results:
                    doi = work.get('doi', '').replace('https://doi.org/', '')
                    if doi:
                        cache_work(doi, work)
                
                # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–º–∏ DOI
                doi_to_work = {w.get('doi', '').replace('https://doi.org/', ''): w for w in results}
                batch_results = []
                
                for doi in uncached_dois:
                    if doi in doi_to_work:
                        batch_results.append(doi_to_work[doi])
                    else:
                        # –ï—Å–ª–∏ —Ä–∞–±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                        try:
                            work_data = await self.fetch_single_work(doi)
                            batch_results.append(work_data)
                        except:
                            batch_results.append(None)
                
                return cached_results + batch_results
            else:
                return cached_results + [None] * len(uncached_dois)
                
        except Exception as e:
            logger.error(f"Batch fetch error: {str(e)}")
            return cached_results + [None] * len(uncached_dois)
    
    async def fetch_single_work(self, doi: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–¥–Ω—É —Ä–∞–±–æ—Ç—É –ø–æ DOI"""
        cached = get_cached_work(doi)
        if cached:
            return cached
        
        url = f"{OPENALEX_BASE_URL}/works/https://doi.org/{doi}"
        data = await self.make_request(url)
        
        if data:
            cache_work(doi, data)
        
        return data
    
    async def fetch_works_by_topic_cursor(self, topic_id: str, max_results: int = 2000) -> List[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–±–æ—Ç—ã –ø–æ —Ç–µ–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º cursor pagination"""
        all_works = []
        cursor = "*"
        page_count = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–π —Ç–µ–º—ã
        cache_key = f"{topic_id}_cursor_{cursor}"
        cached = get_cached_topic_works(topic_id, cache_key)
        
        if cached and len(cached) >= max_results:
            logger.info(f"Using cached data for topic {topic_id}")
            return cached[:max_results]
        
        logger.info(f"Fetching works for topic {topic_id} (max: {max_results})")
        
        try:
            while len(all_works) < max_results and cursor:
                page_count += 1
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º cursor pagination –≤–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–π
                url = (f"{OPENALEX_BASE_URL}/works?"
                      f"filter=topics.id:{topic_id}&"
                      f"per-page={CURSOR_PAGE_SIZE}&"
                      f"cursor={cursor}&"
                      f"sort=publication_date:desc")
                
                data = await self.make_request(url)
                
                if not data or 'results' not in data:
                    break
                
                works = data['results']
                if not works:
                    break
                
                all_works.extend(works)
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π cursor
                meta = data.get('meta', {})
                cursor = meta.get('next_cursor')
                
                logger.info(f"Page {page_count}: got {len(works)} works, total: {len(all_works)}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫—ç—à
                cache_key = f"{topic_id}_cursor_{cursor or 'end'}"
                cache_topic_works(topic_id, cache_key, all_works)
                
                if not cursor or page_count >= 10:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
                    break
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                await asyncio.sleep(0.5)
            
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            result = all_works[:max_results]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            cache_topic_works(topic_id, "final", result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error fetching topic works: {str(e)}")
            return all_works
    
    async def fetch_topic_stats(self, topic_id: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–µ"""
        cached = get_cached_topic_stats(topic_id)
        if cached:
            return cached
        
        url = f"{OPENALEX_BASE_URL}/topics/{topic_id}"
        data = await self.make_request(url)
        
        if data:
            cache_topic_stats(topic_id, data)
        
        return data

# ============================================================================
# –°–ò–ù–•–†–û–ù–ù–´–ï –û–ë–ï–†–¢–ö–ò –î–õ–Ø STREAMLIT
# ============================================================================

def run_async(coro):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∫–æ—Ä—É—Ç–∏–Ω—É –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)
    finally:
        loop.close()

def fetch_works_by_dois_sync(dois: List[str]) -> Tuple[List[dict], int, int]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è batch –∑–∞–ø—Ä–æ—Å–∞ —Ä–∞–±–æ—Ç"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
    batches = [dois[i:i + BATCH_SIZE] for i in range(0, len(dois), BATCH_SIZE)]
    all_results = []
    successful = 0
    failed = 0
    
    async def process_batches():
        nonlocal all_results, successful, failed
        async with OpenAlexAsyncClient() as client:
            for i, batch in enumerate(batches):
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress = (i + 1) / len(batches)
                progress_bar.progress(progress)
                status_text.text(f"–ë–∞—Ç—á {i+1}/{len(batches)}: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batch)} DOI")
                
                # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –±–∞—Ç—á
                results = await client.fetch_works_by_dois_batch(batch)
                
                for result in results:
                    if result:
                        successful += 1
                        all_results.append({
                            'data': result,
                            'success': True
                        })
                    else:
                        failed += 1
                        all_results.append({
                            'data': None,
                            'success': False
                        })
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                if i < len(batches) - 1:
                    await asyncio.sleep(1)
    
    run_async(process_batches())
    
    progress_bar.empty()
    status_text.empty()
    
    return all_results, successful, failed

def fetch_works_by_topic_sync(topic_id: str, max_results: int = 2000) -> List[dict]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ"""
    async def fetch():
        async with OpenAlexAsyncClient() as client:
            return await client.fetch_works_by_topic_cursor(topic_id, max_results)
    
    return run_async(fetch())

def fetch_topic_stats_sync(topic_id: str) -> Optional[dict]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–µ–º—ã"""
    async def fetch():
        async with OpenAlexAsyncClient() as client:
            return await client.fetch_topic_stats(topic_id)
    
    return run_async(fetch())

# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
# ============================================================================

def normalize_word(word: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞"""
    word_lower = word.lower()
    
    if len(word_lower) < 4:
        return ''
    
    plural_exceptions = {
        'analyses': 'analysis', 'bases': 'base', 'criteria': 'criterion',
        'hypotheses': 'hypothesis', 'phenomena': 'phenomenon',
        'properties': 'property', 'activities': 'activity',
        'efficiencies': 'efficiency', 'performances': 'performance'
    }
    
    if word_lower in plural_exceptions:
        return plural_exceptions[word_lower]
    
    if word_lower.endswith('ies'):
        base = word_lower[:-3] + 'y'
        if len(base) >= 4:
            return base
    elif word_lower.endswith('es'):
        if word_lower.endswith(('ches', 'shes', 'xes', 'zes', 'sses')):
            base = word_lower[:-2]
            if len(base) >= 4:
                return base
    elif word_lower.endswith('s') and not word_lower.endswith(('ss', 'us', 'is', 'ys', 'as')):
        base = word_lower[:-1]
        if len(base) >= 4:
            return base
    
    return word_lower

def extract_keywords_from_title(title: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    if not title:
        return []
    
    words = re.findall(r'\b[a-zA-Z]{4,}\b', title)
    filtered_words = []
    
    for word in words:
        word_lower = word.lower()
        
        if word_lower in ALL_STOPWORDS:
            continue
        
        if re.search(r'\d', word_lower):
            continue
        
        normalized = normalize_word(word_lower)
        if normalized:
            filtered_words.append(normalized)
    
    return filtered_words

def parse_doi_input(text: str) -> List[str]:
    """–ü–∞—Ä—Å–∏–Ω–≥ DOI"""
    if not text or not text.strip():
        return []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º DOI —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    doi_pattern = r'10\.\d{4,9}/[-._;()/:A-Za-z0-9]+'
    dois = re.findall(doi_pattern, text, re.IGNORECASE)
    
    # –û—á–∏—Å—Ç–∫–∞
    cleaned_dois = []
    for doi in dois:
        doi = doi.strip()
        if doi:
            if doi.startswith('https://doi.org/'):
                doi = doi[16:]
            elif doi.startswith('http://doi.org/'):
                doi = doi[15:]
            elif doi.startswith('doi.org/'):
                doi = doi[8:]
            cleaned_dois.append(doi)
    
    return list(set(cleaned_dois))[:300]

def analyze_keywords_parallel(titles: List[str]) -> Counter:
    """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    all_keywords = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(extract_keywords_from_title, title) for title in titles]
        for future in as_completed(futures):
            all_keywords.extend(future.result())
    
    return Counter(all_keywords)

def enrich_work_data(work: dict) -> dict:
    """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç—ã"""
    if not work:
        return {}
    
    enriched = {
        'id': work.get('id', ''),
        'doi': work.get('doi', '').replace('https://doi.org/', ''),
        'title': work.get('title', ''),
        'publication_date': work.get('publication_date', ''),
        'publication_year': work.get('publication_year', 0),
        'cited_by_count': work.get('cited_by_count', 0),
        'type': work.get('type', ''),
        'abstract': work.get('abstract', '')[:500] if work.get('abstract') else '',
    }
    
    # –ê–≤—Ç–æ—Ä—ã
    authorships = work.get('authorships', [])
    authors = []
    institutions = set()
    
    for authorship in authorships:
        author = authorship.get('author', {})
        if author:
            author_name = author.get('display_name', '')
            if author_name:
                authors.append(author_name)
        
        for inst in authorship.get('institutions', []):
            inst_name = inst.get('display_name', '')
            if inst_name:
                institutions.add(inst_name)
    
    enriched['authors'] = authors[:5]
    enriched['institutions'] = list(institutions)
    
    # –ñ—É—Ä–Ω–∞–ª
    source = work.get('primary_location', {}).get('source', {})
    enriched['venue_name'] = source.get('display_name', '')
    enriched['venue_type'] = source.get('type', '')
    enriched['is_oa'] = work.get('open_access', {}).get('is_oa', False)
    
    # –¢–µ–º—ã
    topics = work.get('topics', [])
    if topics:
        sorted_topics = sorted(topics, key=lambda x: x.get('score', 0), reverse=True)
        primary_topic = sorted_topics[0]
        enriched['primary_topic'] = primary_topic.get('display_name', '')
        enriched['topic_id'] = primary_topic.get('id', '').split('/')[-1]
    else:
        enriched['primary_topic'] = ''
        enriched['topic_id'] = ''
    
    return enriched

def analyze_works_for_topic(
    topic_id: str,
    keywords: List[str],
    max_citations: int = 10,
    max_works: int = 2000,
    top_n: int = 100
) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ"""
    
    with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ {max_works} —Ä–∞–±–æ—Ç..."):
        works = fetch_works_by_topic_sync(topic_id, max_works)
    
    if not works:
        return []
    
    with st.spinner(f"–ê–Ω–∞–ª–∏–∑ {len(works)} —Ä–∞–±–æ—Ç..."):
        analyzed = []
        
        for work in works:
            cited_by_count = work.get('cited_by_count', 0)
            
            if cited_by_count <= max_citations:
                title = work.get('title', '')
                abstract = work.get('abstract', '')
                
                if title:
                    title_lower = title.lower()
                    abstract_lower = abstract.lower() if abstract else ''
                    
                    score = 0
                    matched = []
                    
                    for keyword in keywords:
                        kw_lower = keyword.lower()
                        if kw_lower in title_lower:
                            score += 3
                            matched.append(keyword)
                        elif abstract and kw_lower in abstract_lower:
                            score += 1
                            matched.append(f"{keyword}*")
                    
                    if score > 0:
                        enriched = enrich_work_data(work)
                        enriched.update({
                            'relevance_score': score,
                            'matched_keywords': matched,
                            'analysis_time': datetime.now().isoformat()
                        })
                        analyzed.append(enriched)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤—ã–±–æ—Ä —Ç–æ–ø-N
        analyzed.sort(key=lambda x: x['relevance_score'], reverse=True)
        return analyzed[:top_n]

def create_filters_ui() -> Dict:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
    with st.sidebar:
        st.header("üéØ –§–∏–ª—å—Ç—Ä—ã")
        
        max_citations = st.slider(
            "–ú–∞–∫—Å. —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π",
            min_value=0,
            max_value=50,
            value=10,
            help="–í–∫–ª—é—á–∞–µ—Ç —Ä–∞–±–æ—Ç—ã —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –∏–ª–∏ –º–µ–Ω—å—à–∏–º —á–∏—Å–ª–æ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"
        )
        
        min_year = st.number_input(
            "–ú–∏–Ω. –≥–æ–¥",
            min_value=1900,
            max_value=datetime.now().year,
            value=2015
        )
        
        venue_types = st.multiselect(
            "–¢–∏–ø –∏–∑–¥–∞–Ω–∏—è",
            options=['journal', 'repository', 'conference', 'book'],
            default=['journal', 'repository']
        )
        
        open_access = st.checkbox("–¢–æ–ª—å–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø", value=False)
        
        min_relevance = st.slider(
            "–ú–∏–Ω. —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å",
            min_value=1,
            max_value=10,
            value=3
        )
    
    return {
        'max_citations': max_citations,
        'min_year': min_year,
        'venue_types': venue_types,
        'open_access': open_access,
        'min_relevance': min_relevance
    }

def apply_filters(works: List[dict], filters: Dict) -> List[dict]:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
    filtered = []
    
    for work in works:
        # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if work.get('cited_by_count', 0) > filters['max_citations']:
            continue
        
        # –ì–æ–¥
        if work.get('publication_year', 0) < filters['min_year']:
            continue
        
        # –¢–∏–ø –∏–∑–¥–∞–Ω–∏—è
        venue_type = work.get('venue_type', '')
        if filters['venue_types'] and venue_type not in filters['venue_types']:
            continue
        
        # –û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø
        if filters['open_access'] and not work.get('is_oa', False):
            continue
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        if work.get('relevance_score', 0) < filters['min_relevance']:
            continue
        
        filtered.append(work)
    
    return filtered

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    st.title("üöÄ CTA Article Recommendation Pro+")
    st.markdown("""
    ### –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∏–∑–∫–æ—Ü–∏—Ç–∏—Ä—É–µ–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    
    **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è:**
    - üöÑ Batch –∑–∞–ø—Ä–æ—Å—ã –∫ OpenAlex API
    - üéØ Cursor pagination –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—ã—Å—è—á —Ä–∞–±–æ—Ç
    - üíæ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ SQLite
    - ‚ö° –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å rate limiting
    """)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
    if 'works_data' not in st.session_state:
        st.session_state.works_data = []
    if 'topic_counter' not in st.session_state:
        st.session_state.topic_counter = Counter()
    if 'keyword_counter' not in st.session_state:
        st.session_state.keyword_counter = Counter()
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞
    clear_old_cache()
    
    # –§–∏–ª—å—Ç—Ä—ã
    filters = create_filters_ui()
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    tab1, tab2, tab3 = st.tabs(["üì• –í–≤–æ–¥ DOI", "üìä –ê–Ω–∞–ª–∏–∑", "üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã"])
    
    with tab1:
        st.subheader("–í–≤–µ–¥–∏—Ç–µ DOI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        doi_input = st.text_area(
            "DOI (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É –∏–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
            height=150,
            placeholder="10.1016/j.jpowsour.2020.228660\n10.1038/s41560-020-00734-0\nhttps://doi.org/10.1021/acsenergylett.1c00123"
        )
        
        if st.button("üöÄ –ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑", type="primary"):
            if doi_input:
                dois = parse_doi_input(doi_input)
                st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(dois)} DOI. –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É...")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–±–æ—Ç
                results, successful, failed = fetch_works_by_dois_sync(dois)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                works_data = []
                topic_counter = Counter()
                titles = []
                
                for result in results:
                    if result.get('success') and result.get('data'):
                        work = result['data']
                        enriched = enrich_work_data(work)
                        
                        if enriched.get('primary_topic'):
                            topic_counter[enriched['primary_topic']] += 1
                        
                        works_data.append(enriched)
                        titles.append(enriched.get('title', ''))
                
                # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                keyword_counter = analyze_keywords_parallel(titles)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏–∏
                st.session_state.works_data = works_data
                st.session_state.topic_counter = topic_counter
                st.session_state.keyword_counter = keyword_counter
                
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {successful} —Ä–∞–±–æ—Ç, –Ω–∞–π–¥–µ–Ω–æ {len(topic_counter)} —Ç–µ–º")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                col1, col2, col3 = st.columns(3)
                col1.metric("–£—Å–ø–µ—à–Ω–æ", successful)
                col2.metric("–¢–µ–º—ã", len(topic_counter))
                col3.metric("–°—Ä–µ–¥–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", 
                          f"{np.mean([w.get('cited_by_count', 0) for w in works_data]):.1f}" if works_data else "0")
    
    with tab2:
        if not st.session_state.works_data:
            st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ DOI –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–í–≤–æ–¥ DOI'")
        else:
            st.subheader("–ê–Ω–∞–ª–∏–∑ —Ç–µ–º")
            
            # –°–ø–∏—Å–æ–∫ —Ç–µ–º
            topics = st.session_state.topic_counter.most_common()
            
            if topics:
                st.write(f"–ù–∞–π–¥–µ–Ω–æ {len(topics)} —Ç–µ–º:")
                
                for i, (topic, count) in enumerate(topics[:20], 1):
                    st.write(f"{i}. **{topic}** - {count} —Ä–∞–±–æ—Ç")
                
                # –í—ã–±–æ—Ä —Ç–µ–º—ã
                topic_options = [f"{topic} ({count} —Ä–∞–±–æ—Ç)" for topic, count in topics]
                selected = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:", topic_options)
                
                if selected:
                    topic_name = selected.split(" (")[0]
                    
                    if st.button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—É", type="primary"):
                        # –ù–∞—Ö–æ–¥–∏–º ID —Ç–µ–º—ã
                        topic_id = None
                        for work in st.session_state.works_data:
                            if work.get('primary_topic') == topic_name:
                                topic_id = work.get('topic_id')
                                break
                        
                        if topic_id:
                            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–º—ã
                            with st.spinner("–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏..."):
                                topic_stats = fetch_topic_stats_sync(topic_id)
                            
                            if topic_stats:
                                total_works = topic_stats.get('works_count', 0)
                                st.metric("–í—Å–µ–≥–æ —Ä–∞–±–æ—Ç –ø–æ —Ç–µ–º–µ", f"{total_works:,}")
                            
                            # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                            top_keywords = [kw for kw, _ in st.session_state.keyword_counter.most_common(15)]
                            
                            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—ã –ø–æ —Ç–µ–º–µ
                            with st.spinner(f"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—Ç..."):
                                relevant_works = analyze_works_for_topic(
                                    topic_id,
                                    [k.lower() for k in top_keywords],
                                    max_citations=filters['max_citations'],
                                    max_works=2000,
                                    top_n=100
                                )
                            
                            st.session_state.selected_topic = topic_name
                            st.session_state.selected_topic_id = topic_id
                            st.session_state.relevant_works = relevant_works
                            
                            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(relevant_works)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—Ç")
                        else:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ ID —Ç–µ–º—ã")
            else:
                st.warning("–¢–µ–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    with tab3:
        if 'relevant_works' not in st.session_state:
            st.info("–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–µ–º—É –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ê–Ω–∞–ª–∏–∑'")
        else:
            works = st.session_state.relevant_works
            topic = st.session_state.get('selected_topic', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
            
            st.subheader(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {topic}")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filtered_works = apply_filters(works, filters)
            
            st.write(f"–ù–∞–π–¥–µ–Ω–æ —Ä–∞–±–æ—Ç: {len(works)} ‚Üí –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {len(filtered_works)}")
            
            if filtered_works:
                # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                display_data = []
                for i, work in enumerate(filtered_works, 1):
                    display_data.append({
                        '‚Ññ': i,
                        '–ó–∞–≥–æ–ª–æ–≤–æ–∫': work.get('title', '')[:100] + '...' if len(work.get('title', '')) > 100 else work.get('title', ''),
                        '–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è': work.get('cited_by_count', 0),
                        '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å': work.get('relevance_score', 0),
                        '–ì–æ–¥': work.get('publication_year', ''),
                        '–ñ—É—Ä–Ω–∞–ª': work.get('venue_name', '')[:30],
                        'DOI': work.get('doi', ''),
                        '–û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø': '‚úÖ' if work.get('is_oa') else '‚ùå'
                    })
                
                df = pd.DataFrame(display_data)
                st.dataframe(df, use_container_width=True, height=500)
                
                # –≠–∫—Å–ø–æ—Ä—Ç
                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å CSV",
                    data=csv,
                    file_name=f"results_{topic.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                st.subheader("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
                col1, col2 = st.columns(2)
                
                with col1:
                    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º
                    citations = [w.get('cited_by_count', 0) for w in filtered_works]
                    fig = px.histogram(x=citations, nbins=20, title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π')
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
                    years = [w.get('publication_year', 0) for w in filtered_works if w.get('publication_year', 0) > 1900]
                    if years:
                        year_counts = pd.Series(years).value_counts().sort_index()
                        fig = px.line(x=year_counts.index, y=year_counts.values, title='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ –≥–æ–¥–∞–º')
                        st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("–ù–µ—Ç —Ä–∞–±–æ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∏–ª—å—Ç—Ä–∞–º")

if __name__ == "__main__":
    main()